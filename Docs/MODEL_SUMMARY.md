# Words Module â€” Model Summary & Technical Specification

> **Last Updated:** February 2026  
> **Module:** `SLR Main/Words/`  
> **Team Reference Document** â€” Read this before running any word notebook

---

## 1. System Overview

The **Words Module** recognizes **whole sign language words** (not individual letters) from video sequences.  
It supports **two languages** using a shared bilingual vocabulary:

| Language | Dataset | Notebook | Status |
|---|---|---|---|
| **English (ASL)** | WLASL (11,980 videos) | `ASL Word (English)/ASL_Word_Training.ipynb` | âœ… Ready to train |
| **Arabic (ArSL)** | KArSL-502 | `ArSL Word (Arabic)/ArSL_Word_Training.ipynb` | â³ Needs KArSL download |

Both models output a **shared `word_id`** (0â€“156), enabling bilingual translation.

---

## 2. Model Architecture

### BiLSTM (Bidirectional Long Short-Term Memory)

```
Input: Video â†’ 30 frames â†’ MediaPipe â†’ (30, 63) tensor
                                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT LAYER           shape = (30, 63)                     â”‚
â”‚    30 time steps Ã— 63 features (21 landmarks Ã— 3 coords)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BIDIRECTIONAL LSTM    128 units (â†’ 256 output)             â”‚
â”‚    Reads sequence forward AND backward                      â”‚
â”‚    return_sequences=True                                    â”‚
â”‚    cuDNN-accelerated (no recurrent_dropout)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BATCH NORMALIZATION                                        â”‚
â”‚  DROPOUT               0.3                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LSTM                  64 units                              â”‚
â”‚    Outputs final hidden state                               â”‚
â”‚    cuDNN-accelerated                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BATCH NORMALIZATION                                        â”‚
â”‚  DROPOUT               0.3                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DENSE                 128 units, ReLU                      â”‚
â”‚    he_normal init, L2 regularization (1e-4)                 â”‚
â”‚  DROPOUT               0.2                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DENSE (OUTPUT)        num_classes units, Softmax           â”‚
â”‚    dtype=float32 (stable with mixed precision)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
            Output: predicted word_id (0â€“156)
```

### Why BiLSTM Instead of MLP?

| | Letters (MLP) | Words (BiLSTM) |
|---|---|---|
| **Input** | Single image â†’ (1, 63) flat | 30-frame video â†’ (30, 63) sequence |
| **Model** | Dense layers only | Temporal layers (LSTM reads over time) |
| **What it learns** | Static hand shape | Hand shape **changes over time** |
| **Example** | ğŸ¤š = letter "B" | ğŸ¤šâ†’âœŠâ†’ğŸ‘‰ = word "help" |

---

## 3. Hyperparameters

| Parameter | Value | Notes |
|---|---|---|
| `SEQUENCE_LENGTH` | 30 | Frames per sample (pad short / sample long) |
| `NUM_FEATURES` | 63 | 21 MediaPipe hand landmarks Ã— 3 (x, y, z) |
| `BATCH_SIZE` | 64 (GPU) / 32 (CPU) | Auto-selected based on hardware |
| `EPOCHS` | 100 max | EarlyStopping will stop sooner |
| `LEARNING_RATE` | 1e-3 | Reduced by ReduceLROnPlateau (Ã—0.5 every 5 stale epochs) |
| `LSTM_UNITS_1` | 128 | BiLSTM layer (outputs 256 due to bidirectional) |
| `LSTM_UNITS_2` | 64 | Second LSTM layer |
| `DENSE_UNITS` | 128 | Classifier hidden layer |
| `DROPOUT_RATE` | 0.3 | Between LSTM layers (0.2 before output) |
| `TEST_SIZE` | 0.4 | â†’ 60% train / 20% val / 20% test |
| `OPTIMIZER` | legacy.Adam | GPU/mixed precision compatible |

---

## 4. GPU Optimizations

| Optimization | What It Does |
|---|---|
| **Memory Growth** | `set_memory_growth(True)` â€” prevents TF from grabbing all VRAM |
| **cuDNN LSTM** | No `recurrent_dropout` â†’ enables NVIDIA cuDNN kernels (5-10Ã— faster) |
| **tf.data Pipeline** | `shuffle â†’ batch â†’ prefetch(AUTOTUNE)` â€” GPU never waits for data |
| **legacy.Adam** | Compatible with mixed precision + GPU placement |
| **clear_session()** | Cleans GPU memory before building model |
| **tf.device(DEVICE)** | Forces model + training onto GPU |
| **L2 + He Init** | Better convergence = fewer epochs needed |
| **Class Weights** | Balanced weighting for imbalanced classes |

---

## 5. Callbacks

| Callback | Config | Purpose |
|---|---|---|
| `ModelCheckpoint` | `monitor='val_accuracy', save_best_only=True` | Saves best model to `*_best.h5` |
| `EarlyStopping` | `monitor='val_loss', patience=15` | Stops training when no improvement |
| `ReduceLROnPlateau` | `factor=0.5, patience=5, min_lr=1e-7` | Halves LR when plateauing |

---

## 6. Output Artifacts

Each notebook produces:

| File | Description |
|---|---|
| `*_word_sequences.npz` | Cached extracted sequences (X, y arrays) â€” skip re-extraction |
| `*_word_lstm_model_best.h5` | Best checkpoint by val_accuracy |
| `*_word_lstm_model_final.h5` | Final model after early stopping |
| `*_word_classes.csv` | Maps model class index â†’ word_id |

---

## 7. Evaluation Metrics

Both notebooks compute:
- **Top-1 Accuracy** â€” exact match
- **Top-5 Accuracy** â€” correct class in top 5 predictions
- **Training curves** â€” accuracy + loss over epochs
- **Confusion matrix** â€” heatmap of class predictions
- **Classification report** â€” precision, recall, F1 per class
- **Per-category accuracy** â€” verb, family, adjective, etc.

---

## 8. Estimated Training Times

| Phase | GPU (RTX-class) | CPU |
|---|---|---|
| MediaPipe extraction (first run) | 30â€“60 min | 2â€“4 hours |
| MediaPipe extraction (cached) | Skipped | Skipped |
| Model training (100 epochs max) | 15â€“30 min | 2â€“4 hours |
| Evaluation | < 1 min | 2â€“5 min |

---

## 9. Parameter Count (Approximate)

```
BiLSTM Layer 1:  ~200K params  (128 units Ã— bidirectional)
LSTM Layer 2:    ~80K params   (64 units)
Dense Layer:     ~16K params   (128 units)
Output Layer:    ~20K params   (157 classes)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:           ~320K trainable parameters
```

This is intentionally lightweight for real-time inference on edge devices.
