{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0c2847",
   "metadata": {},
   "source": [
    "# ASL Word Training ‚Äî Kaggle GPU Optimized (WLASL + MediaPipe + BiLSTM)\n",
    "\n",
    "**Full-parameter Kaggle notebook** for training a word-level ASL recognition model using WLASL videos.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Two-hand detection** (126 features = 2 √ó 21 landmarks √ó 3 coords) ‚Äî captures signs requiring both hands\n",
    "- **GPU-optimized** for Kaggle T4/P100 GPUs\n",
    "- **Full training parameters** ‚Äî no shortcuts, full augmentation pipeline\n",
    "- **Mixed Precision (float16)** ‚Äî faster training on modern GPUs\n",
    "- **tf.data pipeline** with prefetch, shuffle & augmentation\n",
    "- **CuDNN-compatible LSTM** ‚Äî hardware-accelerated RNN\n",
    "- **Temporal Attention** ‚Äî learns which frames matter most\n",
    "- **Label smoothing + gradient clipping** ‚Äî robust training\n",
    "- **Comprehensive evaluation** ‚Äî confusion matrix, per-class F1, category breakdown\n",
    "\n",
    "### Output Artifacts:\n",
    "\n",
    "- `asl_word_sequences_2hand.npz` ‚Äî Extracted two-hand sequences\n",
    "- `asl_word_lstm_model_best.h5` ‚Äî Best model checkpoint\n",
    "- `asl_word_lstm_model_final.h5` ‚Äî Final model\n",
    "- `asl_word_classes.csv` ‚Äî Class mapping\n",
    "\n",
    "### Kaggle Setup:\n",
    "\n",
    "1. Upload `shared_word_vocabulary.csv`, `WLASL_v0.3.json`, `nslt_2000.json`, `missing.txt` as a dataset\n",
    "2. Upload WLASL videos folder as a dataset\n",
    "3. Enable GPU accelerator in notebook settings\n",
    "4. Run all cells in order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ca947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 1: INSTALL & IMPORTS\n",
    "# ===============================\n",
    "# Install mediapipe if not available (Kaggle may not have it)\n",
    "import subprocess\n",
    "import sys\n",
    "try:\n",
    "    import mediapipe\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'mediapipe', '-q'])\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Bidirectional, Dense,\n",
    "    Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚úÖ All libraries imported')\n",
    "print(f'üì¶ TensorFlow : {tf.__version__}')\n",
    "print(f'üì¶ NumPy      : {np.__version__}')\n",
    "print(f'üì¶ OpenCV     : {cv2.__version__}')\n",
    "print(f'üì¶ MediaPipe  : {mp.__version__}')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: GPU DETECTION & CONFIGURATION\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üîç GPU DETECTION & CONFIGURATION')\n",
    "print('=' * 60)\n",
    "print(f'\\nTensorFlow version: {tf.__version__}')\n",
    "print(f'Built with CUDA  : {tf.test.is_built_with_cuda()}')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(f'\\nAll Physical Devices: {physical_devices}')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f'\\nüéÆ GPU Devices Found: {len(gpus)}')\n",
    "\n",
    "USE_GPU = False\n",
    "DEVICE = '/CPU:0'\n",
    "\n",
    "if gpus:\n",
    "    print('\\n‚úÖ GPU IS AVAILABLE!')\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f'   ‚úÖ Memory growth enabled for {len(gpus)} GPU(s)')\n",
    "\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        USE_GPU = True\n",
    "        DEVICE = '/GPU:0'\n",
    "        print(f'   ‚úÖ Using GPU: {gpus[0].name}')\n",
    "\n",
    "        try:\n",
    "            details = tf.config.experimental.get_device_details(gpus[0])\n",
    "            if 'device_name' in details:\n",
    "                print(f'   üìä Device Name       : {details[\"device_name\"]}')\n",
    "            if 'compute_capability' in details:\n",
    "                print(f'   üìä Compute Capability: {details[\"compute_capability\"]}')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f'   ‚ö†Ô∏è  GPU config error: {e}')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è  No GPU detected ‚Äî training on CPU (will be much slower)')\n",
    "\n",
    "# Mixed precision ‚Äî keep OFF for LSTM to avoid NaN\n",
    "ENABLE_MIXED_PRECISION = False\n",
    "\n",
    "if USE_GPU and ENABLE_MIXED_PRECISION:\n",
    "    try:\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print(f'\\n‚ö° Mixed precision enabled: {policy.name}')\n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ö†Ô∏è  Mixed precision not enabled: {e}')\n",
    "else:\n",
    "    mixed_precision.set_global_policy('float32')\n",
    "    print(f'\\nüìê Using float32 precision (stable for LSTM)')\n",
    "\n",
    "# GPU verification test\n",
    "if USE_GPU:\n",
    "    print('\\nüß™ GPU Verification Test...')\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "        print(f'   ‚úÖ GPU computation successful: {c.device}')\n",
    "    except Exception as e:\n",
    "        print(f'   ‚ùå GPU test failed: {e}')\n",
    "        USE_GPU = False\n",
    "        DEVICE = '/CPU:0'\n",
    "\n",
    "print(f'\\n‚úÖ Configuration complete. Using device: {DEVICE}')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 3: CONFIGURATION / PATHS\n",
    "# ===============================\n",
    "# Kaggle paths ‚Äî update these to match your dataset names\n",
    "# On Kaggle, datasets are mounted at /kaggle/input/<dataset-name>/\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # ===== KAGGLE PATHS =====\n",
    "    KAGGLE_INPUT = Path('/kaggle/input')\n",
    "    KAGGLE_OUTPUT = Path('/kaggle/working')\n",
    "    \n",
    "    # Update these dataset names to match YOUR Kaggle dataset uploads:\n",
    "    WLASL_DATASET = KAGGLE_INPUT / 'wlasl-dataset'        # WLASL metadata files\n",
    "    VIDEO_DATASET = KAGGLE_INPUT / 'wlasl-videos'          # WLASL video files\n",
    "    VOCAB_DATASET = KAGGLE_INPUT / 'slr-shared-vocabulary' # shared_word_vocabulary.csv\n",
    "    \n",
    "    SHARED_CSV = VOCAB_DATASET / 'shared_word_vocabulary.csv'\n",
    "    WLASL_JSON = WLASL_DATASET / 'WLASL_v0.3.json'\n",
    "    NSLT_SPLIT = WLASL_DATASET / 'nslt_2000.json'\n",
    "    MISSING_TXT = WLASL_DATASET / 'missing.txt'\n",
    "    VIDEO_DIR = VIDEO_DATASET / 'videos'  # or however your videos are organized\n",
    "    OUTPUT_DIR = KAGGLE_OUTPUT\n",
    "else:\n",
    "    # ===== LOCAL PATHS =====\n",
    "    PROJECT_ROOT = Path(r'E:/Term 9/Grad')\n",
    "    SLR_MAIN = PROJECT_ROOT / 'Main/Sign-Language-Recognition-System-main/SLR Main'\n",
    "    WORDS_ROOT = SLR_MAIN / 'Words'\n",
    "    SHARED_CSV = WORDS_ROOT / 'Shared/shared_word_vocabulary.csv'\n",
    "    WLASL_DIR = PROJECT_ROOT / 'Words dataset'\n",
    "    WLASL_JSON = WLASL_DIR / 'WLASL_v0.3.json'\n",
    "    NSLT_SPLIT = WLASL_DIR / 'nslt_2000.json'\n",
    "    MISSING_TXT = WLASL_DIR / 'missing.txt'\n",
    "    VIDEO_DIR = WORDS_ROOT / 'Datasets/WLASL_videos'\n",
    "    OUTPUT_DIR = WORDS_ROOT / 'ASL Word (English)'\n",
    "\n",
    "# ===== TWO-HAND SEQUENCE PARAMETERS =====\n",
    "SEQUENCE_LENGTH = 30        # frames per sample\n",
    "NUM_HANDS = 2               # detect both hands\n",
    "LANDMARKS_PER_HAND = 63     # 21 landmarks x 3 (x, y, z)\n",
    "NUM_FEATURES = NUM_HANDS * LANDMARKS_PER_HAND  # 126 features\n",
    "\n",
    "# ===== FULL TRAINING HYPERPARAMETERS =====\n",
    "BATCH_SIZE      = 64        # GPU batch size\n",
    "EPOCHS          = 150       # max epochs (early stopping will cut short)\n",
    "LEARNING_RATE   = 5e-4      # initial learning rate\n",
    "LSTM_UNITS_1    = 256       # BiLSTM layer 1 (larger for 126 features)\n",
    "LSTM_UNITS_2    = 128       # BiLSTM layer 2\n",
    "LSTM_UNITS_3    = 64        # LSTM layer 3\n",
    "DENSE_UNITS     = 256       # Dense layer (larger for more capacity)\n",
    "DROPOUT_RATE    = 0.4       # dropout rate\n",
    "LABEL_SMOOTH    = 0.1       # label smoothing\n",
    "GRAD_CLIP_NORM  = 1.0       # gradient clipping\n",
    "L2_REG          = 1e-4      # L2 regularization\n",
    "TEST_SIZE       = 0.4       # val+test fraction -> 60/20/20\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True) if not IS_KAGGLE else None\n",
    "\n",
    "# Verify paths\n",
    "for name, path in [('Shared CSV', SHARED_CSV), ('WLASL JSON', WLASL_JSON),\n",
    "                    ('NSLT Split', NSLT_SPLIT), ('Missing TXT', MISSING_TXT)]:\n",
    "    status = '‚úÖ' if path.exists() else '‚ùå NOT FOUND'\n",
    "    print(f'{status} {name}: {path}')\n",
    "\n",
    "print(f'\\nüìÅ Video dir : {VIDEO_DIR} ({\"exists\" if VIDEO_DIR.exists() else \"NOT FOUND\"})')\n",
    "print(f'üìÅ Output dir: {OUTPUT_DIR}')\n",
    "print(f'\\n‚öôÔ∏è  Sequence length : {SEQUENCE_LENGTH}')\n",
    "print(f'‚öôÔ∏è  Hands           : {NUM_HANDS}')\n",
    "print(f'‚öôÔ∏è  Features/frame  : {NUM_FEATURES}')\n",
    "print(f'‚öôÔ∏è  Batch size      : {BATCH_SIZE}')\n",
    "print(f'‚öôÔ∏è  Max epochs      : {EPOCHS}')\n",
    "print(f'‚öôÔ∏è  Learning rate   : {LEARNING_RATE}')\n",
    "print(f'‚öôÔ∏è  LSTM units      : {LSTM_UNITS_1}/{LSTM_UNITS_2}/{LSTM_UNITS_3}')\n",
    "print(f'‚öôÔ∏è  Dense units     : {DENSE_UNITS}')\n",
    "print(f'üèÉ Running on       : {\"Kaggle\" if IS_KAGGLE else \"Local\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 4: LOAD VOCAB + WLASL METADATA\n",
    "# ===============================\n",
    "\n",
    "vocab_df = pd.read_csv(SHARED_CSV)\n",
    "vocab_df = vocab_df.dropna(subset=['wlasl_class'])\n",
    "vocab_df['wlasl_class'] = vocab_df['wlasl_class'].astype(int)\n",
    "\n",
    "matched_wlasl_classes = set(vocab_df['wlasl_class'].tolist())\n",
    "wlasl_to_wordid = dict(zip(vocab_df['wlasl_class'], vocab_df['word_id']))\n",
    "id_to_english = dict(zip(vocab_df['word_id'].astype(int), vocab_df['english']))\n",
    "\n",
    "with open(NSLT_SPLIT, 'r', encoding='utf-8') as f:\n",
    "    nslt = json.load(f)\n",
    "\n",
    "with open(MISSING_TXT, 'r', encoding='utf-8') as f:\n",
    "    missing_ids = set(x.strip() for x in f if x.strip())\n",
    "\n",
    "with open(WLASL_JSON, 'r', encoding='utf-8') as f:\n",
    "    wlasl_data = json.load(f)\n",
    "\n",
    "# Build download list filtered by shared vocabulary\n",
    "download_list = []\n",
    "for entry in wlasl_data:\n",
    "    gloss = entry.get('gloss', '')\n",
    "    for inst in entry.get('instances', []):\n",
    "        vid = inst.get('video_id')\n",
    "        if not vid or vid not in nslt or vid in missing_ids:\n",
    "            continue\n",
    "        class_id = int(nslt[vid]['action'][0])\n",
    "        if class_id not in matched_wlasl_classes:\n",
    "            continue\n",
    "        download_list.append({\n",
    "            'video_id': vid,\n",
    "            'url': inst.get('url'),\n",
    "            'class_id': class_id,\n",
    "            'word_id': int(wlasl_to_wordid[class_id]),\n",
    "            'gloss': gloss,\n",
    "            'subset': nslt[vid]['subset']\n",
    "        })\n",
    "\n",
    "print(f'üì• Total video candidates: {len(download_list)}')\n",
    "print(f'üè∑Ô∏è Unique matched classes: {len(set(d[\"class_id\"] for d in download_list))}')\n",
    "print(f'üìñ Vocabulary size: {len(vocab_df)} words')\n",
    "print(f'\\nüìã Categories: {sorted(vocab_df[\"category\"].unique())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 5: EXTRACT TWO-HAND MEDIAPIPE SEQUENCES\n",
    "# ===============================\n",
    "# Extracts BOTH hand landmarks from each video frame.\n",
    "# Hands are consistently ordered: Left first, Right second.\n",
    "# If only one hand is detected, the other is zero-padded.\n",
    "# Skip this cell if the .npz file already exists!\n",
    "\n",
    "NPZ_PATH = OUTPUT_DIR / 'asl_word_sequences_2hand.npz'\n",
    "\n",
    "if NPZ_PATH.exists():\n",
    "    print(f'‚è© Dataset already exists, skipping extraction')\n",
    "    data = np.load(NPZ_PATH)\n",
    "    X, y = data['X'], data['y']\n",
    "    print(f'   X: {X.shape}, y: {y.shape}, classes: {len(np.unique(y))}')\n",
    "else:\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands_detector = mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=2,           # detect BOTH hands\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "\n",
    "    def extract_frame_landmarks_2hand(frame):\n",
    "        \"\"\"Extract landmarks for both hands from a single frame.\n",
    "        Returns array of shape (126,) = [left_hand(63) | right_hand(63)].\n",
    "        Hands are ordered by MediaPipe handedness label for consistency.\"\"\"\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands_detector.process(rgb)\n",
    "\n",
    "        left_vec = np.zeros(LANDMARKS_PER_HAND, dtype=np.float32)\n",
    "        right_vec = np.zeros(LANDMARKS_PER_HAND, dtype=np.float32)\n",
    "\n",
    "        if results.multi_hand_landmarks and results.multi_handedness:\n",
    "            for hand_lm, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                label = handedness.classification[0].label  # 'Left' or 'Right'\n",
    "                vec = np.array([[p.x, p.y, p.z] for p in hand_lm.landmark]).flatten()\n",
    "                if label == 'Left':\n",
    "                    left_vec = vec\n",
    "                else:\n",
    "                    right_vec = vec\n",
    "\n",
    "        return np.concatenate([left_vec, right_vec])\n",
    "\n",
    "    def extract_video_landmarks_2hand(video_path, max_frames=SEQUENCE_LENGTH):\n",
    "        \"\"\"Extract two-hand landmarks from video, normalized to fixed length.\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "\n",
    "        rows = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            vec = extract_frame_landmarks_2hand(frame)\n",
    "            rows.append(vec)\n",
    "\n",
    "        cap.release()\n",
    "        if len(rows) == 0:\n",
    "            return None\n",
    "\n",
    "        arr = np.array(rows, dtype=np.float32)\n",
    "        if len(arr) >= max_frames:\n",
    "            idx = np.linspace(0, len(arr) - 1, max_frames, dtype=int)\n",
    "            arr = arr[idx]\n",
    "        else:\n",
    "            pad = np.zeros((max_frames - len(arr), NUM_FEATURES), dtype=np.float32)\n",
    "            arr = np.concatenate([arr, pad], axis=0)\n",
    "        return arr\n",
    "\n",
    "    # Build lookup and find video files\n",
    "    meta_by_id = {d['video_id']: d for d in download_list}\n",
    "    video_files = sorted(VIDEO_DIR.glob('*.mp4'))\n",
    "    print(f'üìÅ Found {len(video_files)} video files in {VIDEO_DIR}')\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for vf in tqdm(video_files, desc='Extracting 2-hand landmarks'):\n",
    "        vid = vf.stem\n",
    "        if vid not in meta_by_id:\n",
    "            continue\n",
    "        seq = extract_video_landmarks_2hand(vf)\n",
    "        if seq is None:\n",
    "            continue\n",
    "        \n",
    "        # Skip mostly blank sequences (<20% hand detection)\n",
    "        non_zero_frames = np.sum(np.any(seq != 0, axis=1))\n",
    "        if non_zero_frames < SEQUENCE_LENGTH * 0.2:\n",
    "            continue\n",
    "        \n",
    "        X_list.append(seq)\n",
    "        y_list.append(meta_by_id[vid]['word_id'])\n",
    "\n",
    "    hands_detector.close()\n",
    "    \n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.int32)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(f'\\nüìä X shape: {X.shape} | y shape: {y.shape} | classes: {len(np.unique(y))}')\n",
    "    print(f'‚è±Ô∏è Extraction time: {duration:.2f}s ({duration/60:.1f} min)')\n",
    "\n",
    "    np.savez_compressed(NPZ_PATH, X=X, y=y)\n",
    "    print(f'üíæ Saved: {NPZ_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca516ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 6: DATA EXPLORATION\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üìä DATA EXPLORATION')\n",
    "print('=' * 60)\n",
    "\n",
    "# Ensure data is loaded\n",
    "if 'X' not in dir() or 'y' not in dir():\n",
    "    data = np.load(NPZ_PATH)\n",
    "    X, y = data['X'], data['y']\n",
    "\n",
    "unique_ids, counts = np.unique(y, return_counts=True)\n",
    "word_names = [id_to_english.get(int(uid), str(uid)) for uid in unique_ids]\n",
    "\n",
    "sort_idx = np.argsort(counts)[::-1]\n",
    "sorted_names  = [word_names[i] for i in sort_idx]\n",
    "sorted_counts = counts[sort_idx]\n",
    "\n",
    "# PLOT 1: Class distribution\n",
    "fig, ax = plt.subplots(figsize=(24, 7))\n",
    "ax.bar(range(len(sorted_names)), sorted_counts, color='steelblue', edgecolor='navy', linewidth=0.3)\n",
    "ax.set_xticks(range(len(sorted_names)))\n",
    "ax.set_xticklabels(sorted_names, rotation=90, fontsize=6)\n",
    "ax.set_xlabel('Word (Sign)', fontsize=12)\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.set_title(f'ASL Word Dataset (2-Hand) ‚Äî {len(unique_ids)} classes, {len(y)} total samples', fontsize=14)\n",
    "ax.axhline(y=np.mean(sorted_counts), color='red', linestyle='--', alpha=0.7, label=f'Mean: {np.mean(sorted_counts):.1f}')\n",
    "ax.axhline(y=np.median(sorted_counts), color='orange', linestyle=':', alpha=0.7, label=f'Median: {np.median(sorted_counts):.1f}')\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüìä Dataset Summary:')\n",
    "print(f'   Total samples    : {len(y)}')\n",
    "print(f'   Total classes    : {len(unique_ids)}')\n",
    "print(f'   Features/frame   : {NUM_FEATURES} ({NUM_HANDS} hands)')\n",
    "print(f'   Min samples/class: {counts.min()} ({word_names[counts.argmin()]})')\n",
    "print(f'   Max samples/class: {counts.max()} ({word_names[counts.argmax()]})')\n",
    "print(f'   Mean             : {counts.mean():.1f}')\n",
    "print(f'   Median           : {np.median(counts):.1f}')\n",
    "\n",
    "# PLOT 2: Quality analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 5))\n",
    "\n",
    "axes[0].hist(sorted_counts, bins=20, color='steelblue', edgecolor='navy', alpha=0.85)\n",
    "axes[0].set_xlabel('Samples per Class', fontsize=11)\n",
    "axes[0].set_ylabel('Number of Classes', fontsize=11)\n",
    "axes[0].set_title('Samples per Class Distribution', fontsize=13)\n",
    "axes[0].axvline(x=np.mean(sorted_counts), color='red', linestyle='--', label=f'Mean: {np.mean(sorted_counts):.1f}')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# Zero-frame quality for left hand\n",
    "left_zero = []\n",
    "right_zero = []\n",
    "for i in range(min(len(X), 1000)):\n",
    "    left_sums = np.sum(np.abs(X[i, :, :LANDMARKS_PER_HAND]), axis=1)\n",
    "    right_sums = np.sum(np.abs(X[i, :, LANDMARKS_PER_HAND:]), axis=1)\n",
    "    left_zero.append(np.sum(left_sums == 0) / SEQUENCE_LENGTH * 100)\n",
    "    right_zero.append(np.sum(right_sums == 0) / SEQUENCE_LENGTH * 100)\n",
    "\n",
    "axes[1].hist(left_zero, bins=20, alpha=0.7, color='#2196F3', edgecolor='navy', label='Left hand')\n",
    "axes[1].hist(right_zero, bins=20, alpha=0.7, color='#FF9800', edgecolor='darkred', label='Right hand')\n",
    "axes[1].set_xlabel('% Zero Frames', fontsize=11)\n",
    "axes[1].set_ylabel('Number of Samples', fontsize=11)\n",
    "axes[1].set_title('Per-Hand Zero Frame Analysis', fontsize=13)\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "# Two-hand detection rate\n",
    "both_hands = []\n",
    "for i in range(min(len(X), 1000)):\n",
    "    left_active = np.any(X[i, :, :LANDMARKS_PER_HAND] != 0, axis=1)\n",
    "    right_active = np.any(X[i, :, LANDMARKS_PER_HAND:] != 0, axis=1)\n",
    "    both_active = np.sum(left_active & right_active) / SEQUENCE_LENGTH * 100\n",
    "    both_hands.append(both_active)\n",
    "\n",
    "axes[2].hist(both_hands, bins=20, color='#4CAF50', edgecolor='darkgreen', alpha=0.85)\n",
    "axes[2].set_xlabel('% Frames with Both Hands', fontsize=11)\n",
    "axes[2].set_ylabel('Number of Samples', fontsize=11)\n",
    "axes[2].set_title(f'Two-Hand Detection Rate (mean: {np.mean(both_hands):.1f}%)', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüñêÔ∏è Two-Hand Stats (first 1000 samples):')\n",
    "print(f'   Mean both-hands frames: {np.mean(both_hands):.1f}%')\n",
    "print(f'   Samples with >50% both: {np.sum(np.array(both_hands) > 50)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 7: PREPROCESSING & DATA SPLITS\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üîß PREPROCESSING & TRAIN/VAL/TEST SPLIT')\n",
    "print('=' * 60)\n",
    "\n",
    "# Reload from cache\n",
    "data = np.load(NPZ_PATH)\n",
    "X, y = data['X'], data['y']\n",
    "\n",
    "# StandardScaler normalization (per-feature)\n",
    "original_shape = X.shape\n",
    "X_flat = X.reshape(-1, NUM_FEATURES)\n",
    "scaler = StandardScaler()\n",
    "X_flat = scaler.fit_transform(X_flat)\n",
    "X = X_flat.reshape(original_shape).astype(np.float32)\n",
    "print(f'   ‚úÖ StandardScaler applied: mean~0, std~1 per feature')\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "num_classes = len(encoder.classes_)\n",
    "y_onehot = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Stratified split: 60/20/20\n",
    "try:\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y_onehot, test_size=TEST_SIZE, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    temp_labels = np.argmax(y_temp, axis=1)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "except ValueError:\n",
    "    print('‚ö†Ô∏è Some classes have too few samples for stratification. Using random split.')\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y_onehot, test_size=TEST_SIZE, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "# Class weights for imbalanced data\n",
    "train_labels = np.argmax(y_train, axis=1)\n",
    "cw = compute_class_weight('balanced', classes=np.arange(num_classes), y=train_labels)\n",
    "cw = np.clip(cw, 0.5, 10.0)  # clip extreme weights\n",
    "class_weights = dict(enumerate(cw))\n",
    "\n",
    "print(f'\\nüìä Split Summary:')\n",
    "print(f'   Classes      : {num_classes}')\n",
    "print(f'   Train        : {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.0f}%)')\n",
    "print(f'   Validation   : {X_val.shape[0]} ({X_val.shape[0]/len(X)*100:.0f}%)')\n",
    "print(f'   Test         : {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.0f}%)')\n",
    "print(f'   Input shape  : {X_train.shape[1:]}')\n",
    "print(f'   Class weights: balanced (clipped [0.5, 10]) ‚Äî max: {max(cw):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 8: BUILD & TRAIN BiLSTM (FULL PARAMETERS)\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üöÄ BUILDING & TRAINING OPTIMIZED BiLSTM MODEL')\n",
    "print('   Two-Hand Mode | Full Parameters | GPU-Accelerated')\n",
    "print('=' * 60)\n",
    "\n",
    "# Clear previous session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "BATCH_SIZE_TRAIN = BATCH_SIZE if USE_GPU else 32\n",
    "print(f'   üì¶ Batch size: {BATCH_SIZE_TRAIN} ({\"GPU\" if USE_GPU else \"CPU\"})')\n",
    "\n",
    "# --- Data Augmentation ---\n",
    "def augment_sequence(x, y):\n",
    "    \"\"\"Apply augmentations to two-hand landmark sequences.\"\"\"\n",
    "    # 1) Gaussian noise\n",
    "    noise = tf.random.normal(shape=tf.shape(x), mean=0.0, stddev=0.005)\n",
    "    x = x + noise\n",
    "\n",
    "    # 2) Random temporal shift: roll frames by +/-3\n",
    "    shift = tf.random.uniform([], minval=-3, maxval=4, dtype=tf.int32)\n",
    "    x = tf.roll(x, shift=shift, axis=0)\n",
    "\n",
    "    # 3) Random frame dropout: zero out ~10% of frames\n",
    "    frame_mask = tf.random.uniform([SEQUENCE_LENGTH, 1]) > 0.1\n",
    "    frame_mask = tf.cast(frame_mask, tf.float32)\n",
    "    x = x * frame_mask\n",
    "\n",
    "    # 4) Random scaling (simulate distance variation)\n",
    "    scale = tf.random.uniform([], 0.85, 1.15)\n",
    "    x = x * scale\n",
    "\n",
    "    # 5) Random hand swap (data augmentation for symmetry)\n",
    "    # Swap left/right hand landmarks with 20% probability\n",
    "    swap = tf.random.uniform([]) < 0.2\n",
    "    if swap:\n",
    "        left = x[:, :LANDMARKS_PER_HAND]\n",
    "        right = x[:, LANDMARKS_PER_HAND:]\n",
    "        x = tf.concat([right, left], axis=1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# --- tf.data Pipelines ---\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.shuffle(buffer_size=min(len(X_train), 10000), seed=42, reshuffle_each_iteration=True)\n",
    "train_ds = train_ds.map(augment_sequence, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.batch(BATCH_SIZE_TRAIN).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE_TRAIN).prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_ds = test_ds.batch(BATCH_SIZE_TRAIN).prefetch(AUTOTUNE)\n",
    "\n",
    "print(f'   ‚úÖ tf.data pipelines with augmentation ready')\n",
    "print(f'   ‚úÖ Class weights for {len(class_weights)} classes')\n",
    "\n",
    "# --- Temporal Attention Layer ---\n",
    "class TemporalAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Learnable attention over time steps.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.nn.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        output = tf.reduce_sum(x * a, axis=1)\n",
    "        return output\n",
    "\n",
    "# --- Build Model (Full Parameters) ---\n",
    "with tf.device(DEVICE):\n",
    "    model = Sequential([\n",
    "        Input(shape=(SEQUENCE_LENGTH, NUM_FEATURES), name='landmark_sequence'),\n",
    "\n",
    "        # BiLSTM block 1 ‚Äî captures broad temporal patterns from both hands\n",
    "        Bidirectional(LSTM(LSTM_UNITS_1, return_sequences=True,\n",
    "                           recurrent_dropout=0.0,\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(L2_REG)),\n",
    "                      name='bilstm_1'),\n",
    "        BatchNormalization(name='bn_1'),\n",
    "        tf.keras.layers.SpatialDropout1D(DROPOUT_RATE, name='sdrop_1'),\n",
    "\n",
    "        # BiLSTM block 2 ‚Äî refines temporal features\n",
    "        Bidirectional(LSTM(LSTM_UNITS_2, return_sequences=True,\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(L2_REG)),\n",
    "                      name='bilstm_2'),\n",
    "        BatchNormalization(name='bn_2'),\n",
    "        tf.keras.layers.SpatialDropout1D(DROPOUT_RATE, name='sdrop_2'),\n",
    "\n",
    "        # LSTM block 3 ‚Äî final temporal encoding\n",
    "        LSTM(LSTM_UNITS_3, return_sequences=True,\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(L2_REG),\n",
    "             name='lstm_3'),\n",
    "        BatchNormalization(name='bn_3'),\n",
    "\n",
    "        # Temporal Attention ‚Äî focus on discriminative frames\n",
    "        TemporalAttention(name='temporal_attention'),\n",
    "        Dropout(DROPOUT_RATE, name='drop_att'),\n",
    "\n",
    "        # Dense classifier head\n",
    "        Dense(DENSE_UNITS, activation='relu',\n",
    "              kernel_initializer='he_normal',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(L2_REG),\n",
    "              name='dense_1'),\n",
    "        BatchNormalization(name='bn_dense'),\n",
    "        Dropout(DROPOUT_RATE, name='drop_dense'),\n",
    "\n",
    "        Dense(DENSE_UNITS // 2, activation='relu',\n",
    "              kernel_initializer='he_normal',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(L2_REG),\n",
    "              name='dense_2'),\n",
    "        BatchNormalization(name='bn_dense_2'),\n",
    "        Dropout(DROPOUT_RATE * 0.5, name='drop_dense_2'),\n",
    "\n",
    "        Dense(num_classes, activation='softmax', dtype='float32', name='output')\n",
    "    ], name='ASL_Word_BiLSTM_2Hand')\n",
    "\n",
    "# Optimizer with gradient clipping\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    clipnorm=GRAD_CLIP_NORM\n",
    ")\n",
    "\n",
    "# Loss with label smoothing\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('\\nüìê Model Architecture:')\n",
    "model.summary()\n",
    "print(f'\\nüñ•Ô∏è  Training on: {DEVICE}')\n",
    "print(f'üñêÔ∏è  Input: {NUM_HANDS} hands x {LANDMARKS_PER_HAND} features = {NUM_FEATURES} total')\n",
    "\n",
    "# --- Callbacks ---\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        str(OUTPUT_DIR / 'asl_word_lstm_model_best.h5'),\n",
    "        monitor='val_accuracy', save_best_only=True, mode='max', verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', patience=25, restore_best_weights=True, verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7, verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.TerminateOnNaN()\n",
    "]\n",
    "\n",
    "# --- Train ---\n",
    "print('\\nüöÄ Starting training...')\n",
    "print(f'   Device       : {DEVICE}')\n",
    "print(f'   Batch size   : {BATCH_SIZE_TRAIN}')\n",
    "print(f'   Max epochs   : {EPOCHS}')\n",
    "print(f'   LR           : {LEARNING_RATE}')\n",
    "print(f'   Label smooth : {LABEL_SMOOTH}')\n",
    "print(f'   Grad clip    : {GRAD_CLIP_NORM}')\n",
    "print(f'   Augmentation : ON (noise + shift + dropout + scale + hand swap)')\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f'\\n‚úÖ Training complete in {training_time:.1f}s ({training_time/60:.1f} min)')\n",
    "\n",
    "# Save final model + class mapping\n",
    "model.save(str(OUTPUT_DIR / 'asl_word_lstm_model_final.h5'))\n",
    "class_df = pd.DataFrame({\n",
    "    'model_class_index': range(num_classes),\n",
    "    'word_id': encoder.classes_.tolist()\n",
    "})\n",
    "class_df.to_csv(OUTPUT_DIR / 'asl_word_classes.csv', index=False)\n",
    "print(f'\\nüíæ Final model : {OUTPUT_DIR / \"asl_word_lstm_model_final.h5\"}')\n",
    "print(f'üíæ Best model  : {OUTPUT_DIR / \"asl_word_lstm_model_best.h5\"}')\n",
    "print(f'üíæ Classes CSV : {OUTPUT_DIR / \"asl_word_classes.csv\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 9: EVALUATION & VISUALIZATION DASHBOARD\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üìà MODEL EVALUATION & VISUALIZATION DASHBOARD')\n",
    "print('=' * 60)\n",
    "\n",
    "# Load best checkpoint\n",
    "best_model = tf.keras.models.load_model(\n",
    "    str(OUTPUT_DIR / 'asl_word_lstm_model_best.h5'),\n",
    "    custom_objects={'TemporalAttention': TemporalAttention}\n",
    ")\n",
    "\n",
    "# Predict\n",
    "eval_batch = 64 if USE_GPU else 32\n",
    "eval_ds = tf.data.Dataset.from_tensor_slices((X_test,)).batch(eval_batch).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    proba = best_model.predict(eval_ds, verbose=0)\n",
    "\n",
    "y_pred = np.argmax(proba, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Top-1 accuracy\n",
    "top1_acc = (y_pred == y_true).mean()\n",
    "\n",
    "# Top-5 accuracy\n",
    "top5_correct = 0\n",
    "for i in range(len(y_true)):\n",
    "    top5 = np.argsort(proba[i])[-5:]\n",
    "    if y_true[i] in top5:\n",
    "        top5_correct += 1\n",
    "top5_acc = top5_correct / len(y_true)\n",
    "\n",
    "print(f'\\nüéØ Test Results (Two-Hand Model):')\n",
    "print(f'   Top-1 Accuracy : {top1_acc:.4f} ({top1_acc*100:.2f}%)')\n",
    "print(f'   Top-5 Accuracy : {top5_acc:.4f} ({top5_acc*100:.2f}%)')\n",
    "print(f'   Test samples   : {len(y_true)}')\n",
    "print(f'   Classes        : {num_classes}')\n",
    "print(f'   Features       : {NUM_FEATURES} ({NUM_HANDS} hands)')\n",
    "\n",
    "# =============================================\n",
    "# PLOT 1: Training Dashboard (4 panels)\n",
    "# =============================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2, color='#2196F3')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2, color='#FF9800')\n",
    "axes[0, 0].fill_between(range(len(history.history['accuracy'])),\n",
    "                         history.history['accuracy'], history.history['val_accuracy'],\n",
    "                         alpha=0.1, color='red')\n",
    "axes[0, 0].set_title('Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylim([0, 1.05])\n",
    "best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "axes[0, 0].axvline(x=best_epoch, color='green', linestyle=':', alpha=0.5)\n",
    "\n",
    "axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2, color='#2196F3')\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2, color='#FF9800')\n",
    "axes[0, 1].fill_between(range(len(history.history['loss'])),\n",
    "                         history.history['loss'], history.history['val_loss'],\n",
    "                         alpha=0.1, color='red')\n",
    "axes[0, 1].set_title('Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "if 'lr' in history.history:\n",
    "    lr_values = history.history['lr']\n",
    "else:\n",
    "    lr_values = [LEARNING_RATE] * len(history.history['loss'])\n",
    "axes[1, 0].plot(lr_values, linewidth=2, color='#4CAF50', marker='o', markersize=3)\n",
    "axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "train_acc = np.array(history.history['accuracy'])\n",
    "val_acc = np.array(history.history['val_accuracy'])\n",
    "gap = train_acc - val_acc\n",
    "axes[1, 1].bar(range(len(gap)), gap, color=['green' if g < 0.05 else 'orange' if g < 0.15 else 'red' for g in gap],\n",
    "               edgecolor='black', linewidth=0.3, alpha=0.8)\n",
    "axes[1, 1].axhline(y=0.05, color='green', linestyle='--', alpha=0.5, label='Healthy gap (5%)')\n",
    "axes[1, 1].axhline(y=0.15, color='red', linestyle='--', alpha=0.5, label='Overfitting (15%)')\n",
    "axes[1, 1].set_title('Overfitting Monitor', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy Gap')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'ASL Word BiLSTM (2-Hand) ‚Äî Top-1: {top1_acc*100:.1f}%, Top-5: {top5_acc*100:.1f}%',\n",
    "             fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 2: Confidence Distribution\n",
    "# =============================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "correct_mask = y_pred == y_true\n",
    "correct_conf = np.max(proba[correct_mask], axis=1)\n",
    "wrong_conf   = np.max(proba[~correct_mask], axis=1) if np.sum(~correct_mask) > 0 else np.array([])\n",
    "\n",
    "axes[0].hist(correct_conf, bins=30, alpha=0.7, color='#4CAF50', edgecolor='darkgreen', label=f'Correct ({len(correct_conf)})')\n",
    "if len(wrong_conf) > 0:\n",
    "    axes[0].hist(wrong_conf, bins=30, alpha=0.7, color='#F44336', edgecolor='darkred', label=f'Wrong ({len(wrong_conf)})')\n",
    "axes[0].set_xlabel('Prediction Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Confidence: Correct vs Wrong', fontsize=13)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "top1_conf = np.max(proba, axis=1)\n",
    "sorted_proba = np.sort(proba, axis=1)[:, ::-1]\n",
    "top2_conf = sorted_proba[:, 1] if proba.shape[1] > 1 else np.zeros(len(proba))\n",
    "margin = top1_conf - top2_conf\n",
    "\n",
    "axes[1].hist(margin, bins=30, color='#9C27B0', edgecolor='purple', alpha=0.8)\n",
    "axes[1].set_xlabel('Confidence Margin (Top-1 - Top-2)', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title(f'Decision Margin ‚Äî Mean: {np.mean(margin):.3f}', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# Classification Report\n",
    "# =============================================\n",
    "word_labels = []\n",
    "for cls_idx in range(num_classes):\n",
    "    wid = int(encoder.classes_[cls_idx])\n",
    "    word_labels.append(id_to_english.get(wid, str(wid)))\n",
    "\n",
    "print('\\nüìã Classification Report:')\n",
    "report = classification_report(y_true, y_pred, labels=range(num_classes), target_names=word_labels, zero_division=0, output_dict=True)\n",
    "print(classification_report(y_true, y_pred, labels=range(num_classes), target_names=word_labels, zero_division=0))\n",
    "\n",
    "# =============================================\n",
    "# PLOT 3: Per-Class F1 Score\n",
    "# =============================================\n",
    "class_f1 = {k: v['f1-score'] for k, v in report.items() if k in word_labels}\n",
    "sorted_f1 = sorted(class_f1.items(), key=lambda x: x[1], reverse=True)\n",
    "f1_names = [x[0] for x in sorted_f1]\n",
    "f1_vals  = [x[1] for x in sorted_f1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 6))\n",
    "colors_f1 = ['#4CAF50' if v >= 0.7 else '#FF9800' if v >= 0.4 else '#F44336' for v in f1_vals]\n",
    "ax.bar(range(len(f1_names)), f1_vals, color=colors_f1, edgecolor='black', linewidth=0.3)\n",
    "ax.set_xticks(range(len(f1_names)))\n",
    "ax.set_xticklabels(f1_names, rotation=90, fontsize=6)\n",
    "ax.set_xlabel('Word', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title(f'Per-Class F1 (green>=0.7, orange>=0.4, red<0.4) ‚Äî Mean: {np.mean(f1_vals):.3f}', fontsize=14)\n",
    "ax.axhline(y=np.mean(f1_vals), color='blue', linestyle='--', alpha=0.5, label=f'Mean F1: {np.mean(f1_vals):.3f}')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 4: Confusion Matrix\n",
    "# =============================================\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 18))\n",
    "if num_classes <= 50:\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=word_labels, yticklabels=word_labels, ax=ax,\n",
    "                linewidths=0.5, linecolor='lightgray')\n",
    "else:\n",
    "    sns.heatmap(cm, annot=False, cmap='Blues',\n",
    "                xticklabels=word_labels, yticklabels=word_labels, ax=ax)\n",
    "ax.set_title(f'Confusion Matrix ‚Äî {num_classes} classes (Top-1: {top1_acc*100:.1f}%)', fontsize=15)\n",
    "ax.set_xlabel('Predicted', fontsize=13)\n",
    "ax.set_ylabel('True', fontsize=13)\n",
    "plt.xticks(rotation=90, fontsize=5)\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 5: Top-10 Most Confused Pairs\n",
    "# =============================================\n",
    "np.fill_diagonal(cm, 0)\n",
    "confused_pairs = []\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        if cm[i, j] > 0:\n",
    "            confused_pairs.append((word_labels[i], word_labels[j], cm[i, j]))\n",
    "confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "top_confused = confused_pairs[:10]\n",
    "\n",
    "if top_confused:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    pair_labels = [f'{p[0]} -> {p[1]}' for p in top_confused]\n",
    "    pair_counts = [p[2] for p in top_confused]\n",
    "    bars = ax.barh(range(len(pair_labels)), pair_counts, color='#E91E63', edgecolor='darkred', alpha=0.85)\n",
    "    ax.set_yticks(range(len(pair_labels)))\n",
    "    ax.set_yticklabels(pair_labels, fontsize=10)\n",
    "    ax.set_xlabel('Misclassification Count', fontsize=12)\n",
    "    ax.set_title('Top-10 Most Confused Pairs', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    for bar, count in zip(bars, pair_counts):\n",
    "        ax.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,\n",
    "                str(count), va='center', fontsize=10, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 6: Per-Category Accuracy\n",
    "# =============================================\n",
    "cat_map = dict(zip(vocab_df['word_id'].astype(int), vocab_df['category']))\n",
    "category_correct, category_total = {}, {}\n",
    "for i in range(len(y_true)):\n",
    "    wid = int(encoder.classes_[y_true[i]])\n",
    "    cat = cat_map.get(wid, 'unknown')\n",
    "    category_total[cat] = category_total.get(cat, 0) + 1\n",
    "    if y_pred[i] == y_true[i]:\n",
    "        category_correct[cat] = category_correct.get(cat, 0) + 1\n",
    "\n",
    "cat_names = sorted(category_total.keys())\n",
    "cat_accs  = [category_correct.get(c, 0) / category_total[c] for c in cat_names]\n",
    "cat_sizes = [category_total[c] for c in cat_names]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "x_pos = range(len(cat_names))\n",
    "bars = ax1.bar(x_pos, [a * 100 for a in cat_accs], color='#2196F3', edgecolor='navy', alpha=0.85)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(cat_names, rotation=45, ha='right', fontsize=11)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_ylim([0, 105])\n",
    "\n",
    "for bar, acc, size in zip(bars, cat_accs, cat_sizes):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{acc*100:.1f}%\\n(n={size})', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax1.set_title('Per-Category Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.axhline(y=top1_acc*100, color='red', linestyle='--', alpha=0.5, label=f'Overall: {top1_acc*100:.1f}%')\n",
    "ax1.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 7: Best & Worst Performing Classes\n",
    "# =============================================\n",
    "per_class_acc = {}\n",
    "for i in range(num_classes):\n",
    "    mask = y_true == i\n",
    "    if mask.sum() > 0:\n",
    "        per_class_acc[word_labels[i]] = (y_pred[mask] == i).mean()\n",
    "\n",
    "sorted_acc = sorted(per_class_acc.items(), key=lambda x: x[1])\n",
    "n_show = min(10, len(sorted_acc))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "worst = sorted_acc[:n_show]\n",
    "ax1.barh(range(len(worst)), [w[1]*100 for w in worst], color='#F44336', edgecolor='darkred', alpha=0.85)\n",
    "ax1.set_yticks(range(len(worst)))\n",
    "ax1.set_yticklabels([w[0] for w in worst], fontsize=10)\n",
    "ax1.set_xlabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title(f'Bottom {n_show} Classes', fontsize=14, fontweight='bold', color='#F44336')\n",
    "ax1.set_xlim([0, 105])\n",
    "for i, w in enumerate(worst):\n",
    "    ax1.text(w[1]*100 + 1, i, f'{w[1]*100:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "best = sorted_acc[-n_show:][::-1]\n",
    "ax2.barh(range(len(best)), [b[1]*100 for b in best], color='#4CAF50', edgecolor='darkgreen', alpha=0.85)\n",
    "ax2.set_yticks(range(len(best)))\n",
    "ax2.set_yticklabels([b[0] for b in best], fontsize=10)\n",
    "ax2.set_xlabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title(f'Top {n_show} Classes', fontsize=14, fontweight='bold', color='#4CAF50')\n",
    "ax2.set_xlim([0, 105])\n",
    "for i, b in enumerate(best):\n",
    "    ax2.text(b[1]*100 + 1, i, f'{b[1]*100:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('Best vs Worst Performing Classes (Two-Hand Model)', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 8: Precision vs Recall Scatter\n",
    "# =============================================\n",
    "precisions = [report[w]['precision'] for w in word_labels if w in report]\n",
    "recalls = [report[w]['recall'] for w in word_labels if w in report]\n",
    "f1s = [report[w]['f1-score'] for w in word_labels if w in report]\n",
    "labels_in_report = [w for w in word_labels if w in report]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(recalls, precisions, c=f1s, cmap='RdYlGn', s=50, edgecolors='black', linewidth=0.5, alpha=0.8)\n",
    "plt.colorbar(scatter, label='F1 Score', ax=ax)\n",
    "ax.set_xlabel('Recall', fontsize=13)\n",
    "ax.set_ylabel('Precision', fontsize=13)\n",
    "ax.set_title('Precision vs Recall per Class (color = F1)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([-0.05, 1.05])\n",
    "ax.set_ylim([-0.05, 1.05])\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i, lbl in enumerate(labels_in_report):\n",
    "    if f1s[i] < 0.3:\n",
    "        ax.annotate(lbl, (recalls[i], precisions[i]), fontsize=7, alpha=0.8,\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('‚úÖ Evaluation Dashboard complete!')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a498e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 10: DOWNLOAD MODELS (Kaggle)\n",
    "# ============================================\n",
    "# Download the trained model files from Kaggle output\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print('üì• Model files available for download in /kaggle/working/:')\n",
    "    for f in OUTPUT_DIR.glob('*'):\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f'   {f.name} ({size_mb:.1f} MB)')\n",
    "    print('\\nüí° Download these files and place them in your local ASL Word (English) folder')\n",
    "    print('   The Live Test notebook will auto-detect the 2-hand model (126 features).')\n",
    "else:\n",
    "    print('üìÅ Model files saved to:')\n",
    "    print(f'   {OUTPUT_DIR}')\n",
    "    for f in OUTPUT_DIR.glob('asl_word_*'):\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f'   {f.name} ({size_mb:.1f} MB)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd352a13",
   "metadata": {},
   "source": [
    "## Kaggle Tips & Troubleshooting\n",
    "\n",
    "| Issue                   | Solution                                                     |\n",
    "| ----------------------- | ------------------------------------------------------------ |\n",
    "| **OOM (Out of Memory)** | Reduce `BATCH_SIZE` to 32 or 16                              |\n",
    "| **No GPU detected**     | Enable GPU in notebook settings (Accelerator dropdown)       |\n",
    "| **Slow extraction**     | Pre-extract sequences locally, upload `.npz` to Kaggle       |\n",
    "| **Low accuracy**        | Increase `EPOCHS`, add more data, tune `LSTM_UNITS`          |\n",
    "| **NaN loss**            | Set `ENABLE_MIXED_PRECISION = False`, reduce `LEARNING_RATE` |\n",
    "| **Dataset not found**   | Check Kaggle dataset names match paths in Cell 3             |\n",
    "\n",
    "### Two-Hand Model vs One-Hand:\n",
    "\n",
    "- **One-hand (63 features)**: Faster extraction, works for single-hand signs\n",
    "- **Two-hand (126 features)**: Captures signs requiring both hands (e.g., 'book', 'help', 'family')\n",
    "- The Live Test notebook auto-detects which mode to use based on the loaded model\n",
    "\n",
    "### Kaggle Dataset Setup:\n",
    "\n",
    "1. Create a dataset with WLASL metadata files (`WLASL_v0.3.json`, `nslt_2000.json`, `missing.txt`)\n",
    "2. Create a dataset with WLASL video files (`.mp4` files)\n",
    "3. Create a dataset with `shared_word_vocabulary.csv`\n",
    "4. Update the dataset names in Cell 3 to match your uploads\n",
    "5. Or pre-extract and upload `asl_word_sequences_2hand.npz` to skip extraction\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
