{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f0f5d2",
   "metadata": {},
   "source": [
    "# ASL Word â€” Live Webcam Testing\n",
    "# Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙ„Ù…Ø§Øª Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© â€” Ø¨Ø« Ù…Ø¨Ø§Ø´Ø± Ù…Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§\n",
    "\n",
    "This notebook lets you **test your trained ASL word model in real-time** using your webcam.\n",
    "\n",
    "Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ± ÙŠØªÙŠØ­ Ù„Ùƒ **Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ ÙƒÙ„Ù…Ø§Øª Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹** Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„ÙˆÙŠØ¨.\n",
    "\n",
    "### How it works / ÙƒÙŠÙ ÙŠØ¹Ù…Ù„:\n",
    "1. **Continuous capture** â†’ MediaPipe extracts hand landmarks every frame / Ø§Ù„ØªÙ‚Ø§Ø· Ù…Ø³ØªÙ…Ø± â†’ MediaPipe ÙŠØ³ØªØ®Ø±Ø¬ Ù†Ù‚Ø§Ø· Ø§Ù„ÙŠØ¯\n",
    "2. **Sliding window** â†’ buffers the last 30 frames into a sequence / Ù†Ø§ÙØ°Ø© Ù…Ù†Ø²Ù„Ù‚Ø© â†’ ÙŠØ®Ø²Ù† Ø¢Ø®Ø± 30 Ø¥Ø·Ø§Ø± ÙÙŠ ØªØ³Ù„Ø³Ù„\n",
    "3. **Prediction** â†’ feeds the sequence to the BiLSTM model every 0.5s / Ø§Ù„ØªÙ†Ø¨Ø¤ â†’ ÙŠØºØ°ÙŠ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ ÙƒÙ„ 0.5 Ø«Ø§Ù†ÙŠØ©\n",
    "4. **Sentence building** â†’ confirmed words are appended to a sentence / Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¬Ù…Ù„Ø© â†’ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¤ÙƒØ¯Ø© ØªÙØ¶Ø§Ù Ù„Ù„Ø¬Ù…Ù„Ø©\n",
    "\n",
    "### Controls / Ø§Ù„ØªØ­ÙƒÙ…:\n",
    "| Key / Ù…ÙØªØ§Ø­ | Action / Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ |\n",
    "|---|---|\n",
    "| `q` | Quit / Ø®Ø±ÙˆØ¬ |\n",
    "| `r` | Reset sentence / Ù…Ø³Ø­ Ø§Ù„Ø¬Ù…Ù„Ø© |\n",
    "| `SPACE` | Add space between words / Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§ÙØ© |\n",
    "| `BACKSPACE` | Delete last word / Ø­Ø°Ù Ø¢Ø®Ø± ÙƒÙ„Ù…Ø© |\n",
    "\n",
    "### Requirements / Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:\n",
    "- Trained model: `asl_word_lstm_model_best.h5`\n",
    "- Class mapping: `asl_word_classes.csv`\n",
    "- Webcam connected / ÙƒØ§Ù…ÙŠØ±Ø§ Ù…ØªØµÙ„Ø©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34079a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 1: IMPORTS & SETUP\n",
    "# Ø§Ù„Ø®Ù„ÙŠØ© 1: Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n",
    "# ===============================\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'OpenCV: {cv2.__version__}')\n",
    "print(f'MediaPipe: {mp.__version__}')\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f'âœ… GPU detected: {gpus[0].name}')\n",
    "else:\n",
    "    print('âš ï¸ No GPU â€” running on CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeff2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 2: CONFIGURATION\n",
    "# Ø§Ù„Ø®Ù„ÙŠØ© 2: Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n",
    "# ===============================\n",
    "\n",
    "PROJECT_ROOT = Path(r'E:/Term 9/Grad')\n",
    "SLR_MAIN = PROJECT_ROOT / 'Main/Sign-Language-Recognition-System-main/SLR Main'\n",
    "WORDS_ROOT = SLR_MAIN / 'Words'\n",
    "OUTPUT_DIR = WORDS_ROOT / 'ASL Word (English)'\n",
    "SHARED_CSV = WORDS_ROOT / 'Shared/shared_word_vocabulary.csv'\n",
    "\n",
    "# Model files\n",
    "MODEL_PATH = OUTPUT_DIR / 'asl_word_lstm_model_best.h5'\n",
    "CLASSES_CSV = OUTPUT_DIR / 'asl_word_classes.csv'\n",
    "\n",
    "# Sequence parameters (must match training)\n",
    "SEQUENCE_LENGTH = 30    # frames per sequence\n",
    "NUM_FEATURES = 63       # 21 landmarks Ã— 3 (x, y, z)\n",
    "\n",
    "# Live inference settings\n",
    "CONFIDENCE_THRESHOLD = 0.35     # minimum confidence to accept a prediction\n",
    "PREDICTION_INTERVAL = 0.5       # seconds between predictions\n",
    "STABILITY_WINDOW = 3            # consecutive same predictions needed to confirm\n",
    "COOLDOWN_TIME = 2.0             # seconds after confirming a word before next\n",
    "\n",
    "# Camera\n",
    "CAMERA_INDEX = 0\n",
    "CAMERA_WIDTH = 1280\n",
    "CAMERA_HEIGHT = 720\n",
    "\n",
    "print(f'ğŸ“‚ Model  : {MODEL_PATH}')\n",
    "print(f'ğŸ“‚ Classes: {CLASSES_CSV}')\n",
    "print(f'ğŸ¬ Sequence: {SEQUENCE_LENGTH} frames Ã— {NUM_FEATURES} features')\n",
    "print(f'ğŸ¯ Confidence threshold: {CONFIDENCE_THRESHOLD}')\n",
    "print(f'ğŸ” Stability window: {STABILITY_WINDOW} predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaf578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 3: LOAD MODEL & VOCABULARY\n",
    "# Ø§Ù„Ø®Ù„ÙŠØ© 3: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…ÙØ±Ø¯Ø§Øª\n",
    "# ===============================\n",
    "\n",
    "# --- Custom layer needed for model loading ---\n",
    "class TemporalAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Temporal attention layer (must match training definition).\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.nn.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        output = tf.reduce_sum(x * a, axis=1)\n",
    "        return output\n",
    "\n",
    "# Load model\n",
    "print('Loading model...')\n",
    "model = tf.keras.models.load_model(\n",
    "    str(MODEL_PATH),\n",
    "    custom_objects={'TemporalAttention': TemporalAttention}\n",
    ")\n",
    "print(f'âœ… Model loaded: {model.name} â€” {model.count_params():,} parameters')\n",
    "\n",
    "# Load class mapping\n",
    "class_df = pd.read_csv(CLASSES_CSV)\n",
    "vocab_df = pd.read_csv(SHARED_CSV)\n",
    "vocab_df = vocab_df.dropna(subset=['wlasl_class'])\n",
    "\n",
    "id_to_english = dict(zip(vocab_df['word_id'].astype(int), vocab_df['english']))\n",
    "id_to_arabic  = dict(zip(vocab_df['word_id'].astype(int), vocab_df['arabic']))\n",
    "id_to_category = dict(zip(vocab_df['word_id'].astype(int), vocab_df['category']))\n",
    "\n",
    "# Build model_index â†’ word name mapping\n",
    "index_to_word = {}\n",
    "index_to_arabic = {}\n",
    "for _, row in class_df.iterrows():\n",
    "    idx = int(row['model_class_index'])\n",
    "    wid = int(row['word_id'])\n",
    "    index_to_word[idx] = id_to_english.get(wid, f'word_{wid}')\n",
    "    index_to_arabic[idx] = id_to_arabic.get(wid, '')\n",
    "\n",
    "num_classes = len(index_to_word)\n",
    "print(f'ğŸ·ï¸ {num_classes} word classes loaded')\n",
    "print(f'\\nğŸ“‹ Sample words: {list(index_to_word.values())[:15]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 4: MEDIAPIPE HAND DETECTOR\n",
    "# Ø§Ù„Ø®Ù„ÙŠØ© 4: ÙƒØ§Ø´Ù Ø§Ù„ÙŠØ¯ Ø¨Ù€ MediaPipe\n",
    "# ===============================\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "def extract_landmarks(frame):\n",
    "    \"\"\"Extract 21 hand landmarks (63 features) from a single frame.\n",
    "    Returns numpy array of shape (63,) or zeros if no hand detected.\"\"\"\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        lm = results.multi_hand_landmarks[0]\n",
    "        vec = np.array([[p.x, p.y, p.z] for p in lm.landmark], dtype=np.float32).flatten()\n",
    "        return vec, results.multi_hand_landmarks[0]\n",
    "    return np.zeros(NUM_FEATURES, dtype=np.float32), None\n",
    "\n",
    "print('âœ… MediaPipe hand detector ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb54aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 5: LIVE WEBCAM TESTING\n",
    "# Ø§Ù„Ø®Ù„ÙŠØ© 5: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¨Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§\n",
    "# ===============================\n",
    "# Run this cell to start the live webcam feed.\n",
    "# Ø´ØºÙ‘Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø®Ù„ÙŠØ© Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù…Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.\n",
    "\n",
    "def run_live_test():\n",
    "    \"\"\"Main live testing loop with sliding window prediction.\"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print('âŒ Cannot open camera!')\n",
    "        return\n",
    "\n",
    "    print('ğŸ“¹ Camera opened. Press Q to quit, R to reset, SPACE to add space, BACKSPACE to delete.')\n",
    "\n",
    "    # --- State variables ---\n",
    "    frame_buffer = deque(maxlen=SEQUENCE_LENGTH)     # sliding window of landmark frames\n",
    "    prediction_history = deque(maxlen=STABILITY_WINDOW)  # recent predictions for stability\n",
    "    sentence_words = []                                # built sentence\n",
    "    current_word = ''                                  # current detected word\n",
    "    current_conf = 0.0                                 # current confidence\n",
    "    last_prediction_time = 0.0\n",
    "    last_confirmed_time = 0.0\n",
    "    hand_detected = False\n",
    "    fps_history = deque(maxlen=30)\n",
    "\n",
    "    # Colors\n",
    "    GREEN = (0, 200, 0)\n",
    "    RED = (0, 0, 200)\n",
    "    BLUE = (200, 100, 0)\n",
    "    WHITE = (255, 255, 255)\n",
    "    BLACK = (0, 0, 0)\n",
    "    YELLOW = (0, 220, 220)\n",
    "    ORANGE = (0, 140, 255)\n",
    "\n",
    "    while True:\n",
    "        frame_start = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "        # --- Extract landmarks ---\n",
    "        landmarks, hand_lm = extract_landmarks(frame)\n",
    "        hand_detected = hand_lm is not None\n",
    "        frame_buffer.append(landmarks)\n",
    "\n",
    "        # --- Draw hand landmarks ---\n",
    "        if hand_lm is not None:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_lm, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "        # --- Predict when buffer is full ---\n",
    "        now = time.time()\n",
    "        if len(frame_buffer) == SEQUENCE_LENGTH and (now - last_prediction_time) >= PREDICTION_INTERVAL:\n",
    "            last_prediction_time = now\n",
    "\n",
    "            # Build sequence\n",
    "            seq = np.array(list(frame_buffer), dtype=np.float32)\n",
    "            seq = np.expand_dims(seq, axis=0)  # (1, 30, 63)\n",
    "\n",
    "            # Check if sequence has enough non-zero frames\n",
    "            non_zero = np.sum(np.any(seq[0] != 0, axis=1))\n",
    "            if non_zero >= SEQUENCE_LENGTH * 0.3:  # at least 30% non-zero frames\n",
    "                proba = model.predict(seq, verbose=0)[0]\n",
    "                pred_idx = np.argmax(proba)\n",
    "                pred_conf = proba[pred_idx]\n",
    "                pred_word = index_to_word.get(pred_idx, '?')\n",
    "\n",
    "                # Top-3 for display\n",
    "                top3_idx = np.argsort(proba)[-3:][::-1]\n",
    "                top3 = [(index_to_word.get(i, '?'), proba[i]) for i in top3_idx]\n",
    "\n",
    "                if pred_conf >= CONFIDENCE_THRESHOLD:\n",
    "                    current_word = pred_word\n",
    "                    current_conf = pred_conf\n",
    "                    prediction_history.append(pred_word)\n",
    "\n",
    "                    # Check stability: same word predicted N times in a row\n",
    "                    if (len(prediction_history) == STABILITY_WINDOW and\n",
    "                        len(set(prediction_history)) == 1 and\n",
    "                        (now - last_confirmed_time) >= COOLDOWN_TIME):\n",
    "                        # Confirm the word!\n",
    "                        sentence_words.append(current_word)\n",
    "                        last_confirmed_time = now\n",
    "                        prediction_history.clear()\n",
    "                        print(f'âœ… Confirmed: \"{current_word}\" ({current_conf:.1%})')\n",
    "                else:\n",
    "                    current_word = ''\n",
    "                    current_conf = 0.0\n",
    "            else:\n",
    "                current_word = ''\n",
    "                current_conf = 0.0\n",
    "\n",
    "        # --- Draw UI Overlay ---\n",
    "\n",
    "        # Top bar: prediction info\n",
    "        cv2.rectangle(frame, (0, 0), (w, 90), BLACK, -1)\n",
    "        cv2.rectangle(frame, (0, 0), (w, 90), WHITE, 2)\n",
    "\n",
    "        if current_word:\n",
    "            color = GREEN if current_conf >= 0.6 else YELLOW if current_conf >= 0.4 else ORANGE\n",
    "            cv2.putText(frame, f'Word: {current_word}', (15, 35),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)\n",
    "            cv2.putText(frame, f'Confidence: {current_conf:.1%}', (15, 65),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "            # Confidence bar\n",
    "            bar_x = 450\n",
    "            bar_w = 200\n",
    "            bar_h = 20\n",
    "            cv2.rectangle(frame, (bar_x, 20), (bar_x + bar_w, 20 + bar_h), (50, 50, 50), -1)\n",
    "            fill_w = int(bar_w * current_conf)\n",
    "            cv2.rectangle(frame, (bar_x, 20), (bar_x + fill_w, 20 + bar_h), color, -1)\n",
    "            cv2.rectangle(frame, (bar_x, 20), (bar_x + bar_w, 20 + bar_h), WHITE, 1)\n",
    "\n",
    "            # Stability progress\n",
    "            stable_count = sum(1 for p in prediction_history if p == current_word)\n",
    "            cv2.putText(frame, f'Stability: {stable_count}/{STABILITY_WINDOW}',\n",
    "                        (bar_x, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, WHITE, 1)\n",
    "        else:\n",
    "            status = 'Show a sign...' if hand_detected else 'No hand detected'\n",
    "            cv2.putText(frame, status, (15, 45),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (150, 150, 150), 2)\n",
    "\n",
    "        # Top-3 predictions (right side)\n",
    "        if current_word and 'top3' in dir():\n",
    "            tx = w - 320\n",
    "            cv2.putText(frame, 'Top 3:', (tx, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 1)\n",
    "            for rank, (tw, tc) in enumerate(top3):\n",
    "                y_pos = 45 + rank * 20\n",
    "                cv2.putText(frame, f'{rank+1}. {tw} ({tc:.1%})', (tx, y_pos),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 1)\n",
    "\n",
    "        # Bottom bar: sentence\n",
    "        sentence_text = ' '.join(sentence_words) if sentence_words else '(sentence will appear here)'\n",
    "        cv2.rectangle(frame, (0, h - 55), (w, h), BLACK, -1)\n",
    "        cv2.rectangle(frame, (0, h - 55), (w, h), WHITE, 2)\n",
    "        cv2.putText(frame, f'Sentence: {sentence_text}', (15, h - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, WHITE, 2)\n",
    "\n",
    "        # Buffer indicator (bottom-left)\n",
    "        buf_fill = len(frame_buffer) / SEQUENCE_LENGTH\n",
    "        buf_color = GREEN if buf_fill >= 1.0 else YELLOW\n",
    "        cv2.putText(frame, f'Buffer: {len(frame_buffer)}/{SEQUENCE_LENGTH}',\n",
    "                    (15, h - 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, buf_color, 1)\n",
    "\n",
    "        # Hand status indicator\n",
    "        hand_color = GREEN if hand_detected else RED\n",
    "        hand_text = 'HAND OK' if hand_detected else 'NO HAND'\n",
    "        cv2.circle(frame, (w - 80, h - 75), 8, hand_color, -1)\n",
    "        cv2.putText(frame, hand_text, (w - 170, h - 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, hand_color, 1)\n",
    "\n",
    "        # FPS counter\n",
    "        fps = 1.0 / max(time.time() - frame_start, 1e-6)\n",
    "        fps_history.append(fps)\n",
    "        avg_fps = sum(fps_history) / len(fps_history)\n",
    "        cv2.putText(frame, f'FPS: {avg_fps:.0f}', (w - 110, 115),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, WHITE, 1)\n",
    "\n",
    "        # Cooldown indicator\n",
    "        cooldown_remaining = max(0, COOLDOWN_TIME - (now - last_confirmed_time))\n",
    "        if cooldown_remaining > 0:\n",
    "            cv2.putText(frame, f'Cooldown: {cooldown_remaining:.1f}s',\n",
    "                        (w // 2 - 80, 115), cv2.FONT_HERSHEY_SIMPLEX, 0.6, ORANGE, 2)\n",
    "\n",
    "        # --- Show frame ---\n",
    "        cv2.imshow('ASL Word Recognition â€” Live Test', frame)\n",
    "\n",
    "        # --- Handle keyboard ---\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('r'):\n",
    "            sentence_words.clear()\n",
    "            prediction_history.clear()\n",
    "            current_word = ''\n",
    "            print('ğŸ”„ Sentence reset')\n",
    "        elif key == 32:  # SPACE\n",
    "            sentence_words.append(' ')\n",
    "            print('   [space added]')\n",
    "        elif key == 8:   # BACKSPACE\n",
    "            if sentence_words:\n",
    "                removed = sentence_words.pop()\n",
    "                print(f'â¬…ï¸ Removed: \"{removed}\"')\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    final_sentence = ' '.join(sentence_words)\n",
    "    print(f'\\nğŸ“ Final sentence: {final_sentence}')\n",
    "    return final_sentence\n",
    "\n",
    "# --- RUN ---\n",
    "result = run_live_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217a6ef",
   "metadata": {},
   "source": [
    "## Tips / Ù†ØµØ§Ø¦Ø­\n",
    "\n",
    "| Issue / Ø§Ù„Ù…Ø´ÙƒÙ„Ø© | Solution / Ø§Ù„Ø­Ù„ |\n",
    "|---|---|\n",
    "| **Low FPS** | Close other apps, reduce `CAMERA_WIDTH`/`CAMERA_HEIGHT` / Ù‚Ù„Ù„ Ø¯Ù‚Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ |\n",
    "| **Wrong predictions** | Hold the sign steadily for ~2 seconds / Ø«Ø¨Ù‘Øª Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù…Ø¯Ø© Ø«Ø§Ù†ÙŠØªÙŠÙ† |\n",
    "| **Camera not opening** | Change `CAMERA_INDEX` to 1 or 2 / ØºÙŠÙ‘Ø± Ø±Ù‚Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ |\n",
    "| **Too sensitive** | Increase `STABILITY_WINDOW` to 4-5 / Ø²ÙˆÙ‘Ø¯ Ù†Ø§ÙØ°Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± |\n",
    "| **Not detecting** | Lower `CONFIDENCE_THRESHOLD` to 0.25 / Ù‚Ù„Ù„ Ø­Ø¯ Ø§Ù„Ø«Ù‚Ø© |\n",
    "| **Too slow between words** | Decrease `COOLDOWN_TIME` to 1.0 / Ù‚Ù„Ù„ ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± |\n",
    "\n",
    "### How to perform a sign / ÙƒÙŠÙ ØªØ¤Ø¯ÙŠ Ø¥Ø´Ø§Ø±Ø©:\n",
    "1. Face the camera with your hand clearly visible / ÙˆØ¬Ù‘Ù‡ ÙŠØ¯Ùƒ Ù„Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¨ÙˆØ¶ÙˆØ­\n",
    "2. Perform the sign gesture smoothly / Ø£Ø¯ÙÙ‘ Ø­Ø±ÙƒØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø¨Ø³Ù„Ø§Ø³Ø©\n",
    "3. Wait for the stability bar to fill up / Ø§Ù†ØªØ¸Ø± Ø­ØªÙ‰ ÙŠÙ…ØªÙ„Ø¦ Ø´Ø±ÙŠØ· Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±\n",
    "4. The word will be confirmed and added to the sentence / Ø§Ù„ÙƒÙ„Ù…Ø© Ø³ØªÙØ¤ÙƒØ¯ ÙˆØªÙØ¶Ø§Ù Ù„Ù„Ø¬Ù…Ù„Ø©"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
