{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d8482a",
   "metadata": {},
   "source": [
    "# ArSL Word — Live Webcam Testing\n",
    "\n",
    "This notebook lets you **test your trained Arabic sign language word model in real-time** using your webcam.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Continuous capture** — MediaPipe extracts hand landmarks every frame (supports 1 or 2 hands)\n",
    "2. **Sliding window** — buffers the last 30 frames into a sequence\n",
    "3. **Prediction** — feeds the sequence to the BiLSTM model every 0.5s\n",
    "4. **Sentence building** — confirmed words are appended to a sentence\n",
    "\n",
    "### Two-Hand Support:\n",
    "\n",
    "- **Auto-detects** the model's expected input shape (63 or 126 features)\n",
    "- If the model expects **63 features** (1 hand) — uses the dominant hand only\n",
    "- If the model expects **126 features** (2 hands) — captures both hands and concatenates landmarks\n",
    "- Many Arabic sign language word signs require two hands for proper recognition\n",
    "\n",
    "### Controls:\n",
    "\n",
    "| Key         | Action                  |\n",
    "| ----------- | ----------------------- |\n",
    "| `q`         | Quit                    |\n",
    "| `r`         | Reset sentence          |\n",
    "| `SPACE`     | Add space between words |\n",
    "| `BACKSPACE` | Delete last word        |\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "- Trained model: `arsl_word_lstm_model_best.h5`\n",
    "- Class mapping: `arsl_word_classes.csv`\n",
    "- Webcam connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb80c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 1: IMPORTS & SETUP\n",
    "# ===============================\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'OpenCV: {cv2.__version__}')\n",
    "print(f'MediaPipe: {mp.__version__}')\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f'GPU detected: {gpus[0].name}')\n",
    "else:\n",
    "    print('No GPU — running on CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f666d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 2: CONFIGURATION\n",
    "# ===============================\n",
    "\n",
    "PROJECT_ROOT = Path(r'E:/Term 9/Grad')\n",
    "SLR_MAIN = PROJECT_ROOT / 'Main/Sign-Language-Recognition-System-main/SLR Main'\n",
    "WORDS_ROOT = SLR_MAIN / 'Words'\n",
    "OUTPUT_DIR = WORDS_ROOT / 'ArSL Word (Arabic)'\n",
    "SHARED_CSV = WORDS_ROOT / 'Shared/shared_word_vocabulary.csv'\n",
    "\n",
    "# Model files\n",
    "MODEL_PATH = OUTPUT_DIR / 'arsl_word_lstm_model_best.h5'\n",
    "CLASSES_CSV = OUTPUT_DIR / 'arsl_word_classes.csv'\n",
    "\n",
    "# Sequence parameters (must match training)\n",
    "SEQUENCE_LENGTH = 30    # frames per sequence\n",
    "\n",
    "# Hand detection mode: auto-detected from model input shape\n",
    "# - 63 features = 1 hand (21 landmarks x 3)\n",
    "# - 126 features = 2 hands (2 x 21 landmarks x 3)\n",
    "# Set to None for auto-detection, or override manually:\n",
    "NUM_FEATURES = None  # will be set after model loads\n",
    "\n",
    "# Live inference settings\n",
    "CONFIDENCE_THRESHOLD = 0.35     # minimum confidence to accept a prediction\n",
    "PREDICTION_INTERVAL = 0.5       # seconds between predictions\n",
    "STABILITY_WINDOW = 3            # consecutive same predictions needed to confirm\n",
    "COOLDOWN_TIME = 2.0             # seconds after confirming a word before next\n",
    "\n",
    "# Camera\n",
    "CAMERA_INDEX = 0\n",
    "CAMERA_WIDTH = 1280\n",
    "CAMERA_HEIGHT = 720\n",
    "\n",
    "print(f'Model  : {MODEL_PATH}')\n",
    "print(f'Classes: {CLASSES_CSV}')\n",
    "print(f'Sequence: {SEQUENCE_LENGTH} frames')\n",
    "print(f'Confidence threshold: {CONFIDENCE_THRESHOLD}')\n",
    "print(f'Stability window: {STABILITY_WINDOW} predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91482b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 3: LOAD MODEL & VOCABULARY\n",
    "# ===============================\n",
    "\n",
    "# --- Custom layer needed for model loading ---\n",
    "class TemporalAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Temporal attention layer (must match training definition).\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.nn.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        output = tf.reduce_sum(x * a, axis=1)\n",
    "        return output\n",
    "\n",
    "# Load model\n",
    "print('Loading model...')\n",
    "model = tf.keras.models.load_model(\n",
    "    str(MODEL_PATH),\n",
    "    custom_objects={'TemporalAttention': TemporalAttention}\n",
    ")\n",
    "print(f'Model loaded: {model.name} — {model.count_params():,} parameters')\n",
    "\n",
    "# Auto-detect feature count from model input shape\n",
    "model_input_shape = model.input_shape  # (None, SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "NUM_FEATURES = model_input_shape[-1]\n",
    "NUM_HANDS = 2 if NUM_FEATURES == 126 else 1\n",
    "LANDMARKS_PER_HAND = 21 * 3  # 63\n",
    "\n",
    "print(f'Model expects {NUM_FEATURES} features -> {NUM_HANDS} hand(s) mode')\n",
    "\n",
    "# Load class mapping\n",
    "class_df = pd.read_csv(CLASSES_CSV)\n",
    "vocab_df = pd.read_csv(SHARED_CSV)\n",
    "vocab_df = vocab_df.dropna(subset=['karsl_class'])\n",
    "\n",
    "id_to_english = dict(zip(vocab_df['word_id'].astype(int), vocab_df['english']))\n",
    "id_to_arabic = dict(zip(vocab_df['word_id'].astype(int), vocab_df['arabic']))\n",
    "id_to_category = dict(zip(vocab_df['word_id'].astype(int), vocab_df['category']))\n",
    "\n",
    "# Build model_index -> word name mapping (both English and Arabic)\n",
    "index_to_english = {}\n",
    "index_to_arabic = {}\n",
    "for _, row in class_df.iterrows():\n",
    "    idx = int(row['model_class_index'])\n",
    "    wid = int(row['word_id'])\n",
    "    index_to_english[idx] = id_to_english.get(wid, f'word_{wid}')\n",
    "    index_to_arabic[idx] = id_to_arabic.get(wid, f'word_{wid}')\n",
    "\n",
    "num_classes = len(index_to_english)\n",
    "print(f'{num_classes} word classes loaded')\n",
    "print(f'\\nSample words:')\n",
    "for i in list(index_to_english.keys())[:10]:\n",
    "    print(f'   {i}: {index_to_english[i]} / {index_to_arabic[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d06bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 4: MEDIAPIPE HAND DETECTOR\n",
    "# ===============================\n",
    "# Supports both 1-hand and 2-hand detection based on model requirements\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=NUM_HANDS,       # dynamically set based on model\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "def extract_landmarks(frame):\n",
    "    \"\"\"Extract hand landmarks from a single frame.\n",
    "\n",
    "    - 1-hand mode (63 features): returns landmarks for the first detected hand.\n",
    "    - 2-hand mode (126 features): returns concatenated landmarks for both hands.\n",
    "      If only one hand is detected, the other hand's landmarks are zero-padded.\n",
    "      Hands are ordered: Left hand first, Right hand second (consistent ordering).\n",
    "\n",
    "    Returns: (feature_vector, list_of_hand_landmarks_for_drawing)\n",
    "    \"\"\"\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    draw_landmarks = []\n",
    "\n",
    "    if NUM_HANDS == 1:\n",
    "        # Single-hand mode (63 features)\n",
    "        if results.multi_hand_landmarks:\n",
    "            lm = results.multi_hand_landmarks[0]\n",
    "            vec = np.array([[p.x, p.y, p.z] for p in lm.landmark], dtype=np.float32).flatten()\n",
    "            draw_landmarks = [lm]\n",
    "            return vec, draw_landmarks\n",
    "        return np.zeros(NUM_FEATURES, dtype=np.float32), draw_landmarks\n",
    "\n",
    "    else:\n",
    "        # Two-hand mode (126 features)\n",
    "        left_vec = np.zeros(LANDMARKS_PER_HAND, dtype=np.float32)\n",
    "        right_vec = np.zeros(LANDMARKS_PER_HAND, dtype=np.float32)\n",
    "\n",
    "        if results.multi_hand_landmarks and results.multi_handedness:\n",
    "            for hand_lm, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                draw_landmarks.append(hand_lm)\n",
    "                label = handedness.classification[0].label  # 'Left' or 'Right'\n",
    "                vec = np.array([[p.x, p.y, p.z] for p in hand_lm.landmark], dtype=np.float32).flatten()\n",
    "\n",
    "                # Note: MediaPipe labels are mirrored (camera mirror effect)\n",
    "                # 'Left' in MediaPipe = right hand in real life (when image is flipped)\n",
    "                if label == 'Left':\n",
    "                    left_vec = vec\n",
    "                else:\n",
    "                    right_vec = vec\n",
    "\n",
    "        # Concatenate: [left_hand(63) | right_hand(63)] = 126 features\n",
    "        combined = np.concatenate([left_vec, right_vec])\n",
    "        return combined, draw_landmarks\n",
    "\n",
    "print(f'MediaPipe hand detector ready ({NUM_HANDS} hand(s) mode)')\n",
    "print(f'   Features per frame: {NUM_FEATURES}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CELL 5: LIVE WEBCAM TESTING\n",
    "# ===============================\n",
    "# Run this cell to start the live webcam feed.\n",
    "\n",
    "def run_live_test():\n",
    "    \"\"\"Main live testing loop with sliding window prediction.\n",
    "    Supports both 1-hand and 2-hand models automatically.\n",
    "    Displays both English and Arabic translations.\"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print('Cannot open camera!')\n",
    "        return\n",
    "\n",
    "    hand_mode_str = f'{NUM_HANDS} hand(s), {NUM_FEATURES} features'\n",
    "    print(f'Camera opened [{hand_mode_str}]. Press Q to quit, R to reset, SPACE to add space, BACKSPACE to delete.')\n",
    "\n",
    "    # --- State variables ---\n",
    "    frame_buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    prediction_history = deque(maxlen=STABILITY_WINDOW)\n",
    "    sentence_words_en = []\n",
    "    sentence_words_ar = []\n",
    "    current_word_en = ''\n",
    "    current_word_ar = ''\n",
    "    current_conf = 0.0\n",
    "    last_prediction_time = 0.0\n",
    "    last_confirmed_time = 0.0\n",
    "    hand_detected = False\n",
    "    hands_count = 0\n",
    "    fps_history = deque(maxlen=30)\n",
    "\n",
    "    # Colors\n",
    "    GREEN = (0, 200, 0)\n",
    "    RED = (0, 0, 200)\n",
    "    BLUE = (200, 100, 0)\n",
    "    WHITE = (255, 255, 255)\n",
    "    BLACK = (0, 0, 0)\n",
    "    YELLOW = (0, 220, 220)\n",
    "    ORANGE = (0, 140, 255)\n",
    "\n",
    "    while True:\n",
    "        frame_start = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "        # --- Extract landmarks ---\n",
    "        landmarks, hand_lm_list = extract_landmarks(frame)\n",
    "        hand_detected = len(hand_lm_list) > 0\n",
    "        hands_count = len(hand_lm_list)\n",
    "        frame_buffer.append(landmarks)\n",
    "\n",
    "        # --- Draw hand landmarks (all detected hands) ---\n",
    "        for hand_lm in hand_lm_list:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_lm, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "        # --- Predict when buffer is full ---\n",
    "        now = time.time()\n",
    "        if len(frame_buffer) == SEQUENCE_LENGTH and (now - last_prediction_time) >= PREDICTION_INTERVAL:\n",
    "            last_prediction_time = now\n",
    "\n",
    "            # Build sequence\n",
    "            seq = np.array(list(frame_buffer), dtype=np.float32)\n",
    "            seq = np.expand_dims(seq, axis=0)  # (1, 30, NUM_FEATURES)\n",
    "\n",
    "            # Check if sequence has enough non-zero frames\n",
    "            non_zero = np.sum(np.any(seq[0] != 0, axis=1))\n",
    "            if non_zero >= SEQUENCE_LENGTH * 0.3:  # at least 30% non-zero frames\n",
    "                proba = model.predict(seq, verbose=0)[0]\n",
    "                pred_idx = np.argmax(proba)\n",
    "                pred_conf = proba[pred_idx]\n",
    "                pred_word_en = index_to_english.get(pred_idx, '?')\n",
    "                pred_word_ar = index_to_arabic.get(pred_idx, '?')\n",
    "\n",
    "                # Top-3 for display\n",
    "                top3_idx = np.argsort(proba)[-3:][::-1]\n",
    "                top3 = [(index_to_english.get(i, '?'), index_to_arabic.get(i, '?'), proba[i]) for i in top3_idx]\n",
    "\n",
    "                if pred_conf >= CONFIDENCE_THRESHOLD:\n",
    "                    current_word_en = pred_word_en\n",
    "                    current_word_ar = pred_word_ar\n",
    "                    current_conf = pred_conf\n",
    "                    prediction_history.append(pred_word_en)\n",
    "\n",
    "                    # Check stability: same word predicted N times in a row\n",
    "                    if (len(prediction_history) == STABILITY_WINDOW and\n",
    "                        len(set(prediction_history)) == 1 and\n",
    "                        (now - last_confirmed_time) >= COOLDOWN_TIME):\n",
    "                        # Confirm the word!\n",
    "                        sentence_words_en.append(current_word_en)\n",
    "                        sentence_words_ar.append(current_word_ar)\n",
    "                        last_confirmed_time = now\n",
    "                        prediction_history.clear()\n",
    "                        print(f'Confirmed: \"{current_word_en}\" / \"{current_word_ar}\" ({current_conf:.1%})')\n",
    "                else:\n",
    "                    current_word_en = ''\n",
    "                    current_word_ar = ''\n",
    "                    current_conf = 0.0\n",
    "            else:\n",
    "                current_word_en = ''\n",
    "                current_word_ar = ''\n",
    "                current_conf = 0.0\n",
    "\n",
    "        # --- Draw UI Overlay ---\n",
    "\n",
    "        # Top bar: prediction info\n",
    "        cv2.rectangle(frame, (0, 0), (w, 90), BLACK, -1)\n",
    "        cv2.rectangle(frame, (0, 0), (w, 90), WHITE, 2)\n",
    "\n",
    "        if current_word_en:\n",
    "            color = GREEN if current_conf >= 0.6 else YELLOW if current_conf >= 0.4 else ORANGE\n",
    "            cv2.putText(frame, f'Word: {current_word_en}', (15, 35),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)\n",
    "            cv2.putText(frame, f'Confidence: {current_conf:.1%}', (15, 65),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "            # Confidence bar\n",
    "            bar_x = 450\n",
    "            bar_w = 200\n",
    "            bar_h = 20\n",
    "            cv2.rectangle(frame, (bar_x, 20), (bar_x + bar_w, 20 + bar_h), (50, 50, 50), -1)\n",
    "            fill_w = int(bar_w * current_conf)\n",
    "            cv2.rectangle(frame, (bar_x, 20), (bar_x + fill_w, 20 + bar_h), color, -1)\n",
    "            cv2.rectangle(frame, (bar_x, 20), (bar_x + bar_w, 20 + bar_h), WHITE, 1)\n",
    "\n",
    "            # Stability progress\n",
    "            stable_count = sum(1 for p in prediction_history if p == current_word_en)\n",
    "            cv2.putText(frame, f'Stability: {stable_count}/{STABILITY_WINDOW}',\n",
    "                        (bar_x, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, WHITE, 1)\n",
    "        else:\n",
    "            status = 'Show a sign...' if hand_detected else 'No hand detected'\n",
    "            cv2.putText(frame, status, (15, 45),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (150, 150, 150), 2)\n",
    "\n",
    "        # Top-3 predictions (right side) — shows both English and Arabic\n",
    "        if current_word_en and 'top3' in dir():\n",
    "            tx = w - 380\n",
    "            cv2.putText(frame, 'Top 3:', (tx, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 1)\n",
    "            for rank, (tw_en, tw_ar, tc) in enumerate(top3):\n",
    "                y_pos = 45 + rank * 20\n",
    "                cv2.putText(frame, f'{rank+1}. {tw_en} ({tc:.1%})', (tx, y_pos),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 1)\n",
    "\n",
    "        # Bottom bar: sentence (English)\n",
    "        sentence_en = ' '.join(sentence_words_en) if sentence_words_en else '(sentence will appear here)'\n",
    "        sentence_ar = ' '.join(sentence_words_ar) if sentence_words_ar else ''\n",
    "        cv2.rectangle(frame, (0, h - 75), (w, h), BLACK, -1)\n",
    "        cv2.rectangle(frame, (0, h - 75), (w, h), WHITE, 2)\n",
    "        cv2.putText(frame, f'EN: {sentence_en}', (15, h - 45),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, WHITE, 2)\n",
    "        if sentence_ar:\n",
    "            cv2.putText(frame, f'AR: {sentence_ar}', (15, h - 15),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 255, 100), 2)\n",
    "\n",
    "        # Buffer indicator (above bottom bar)\n",
    "        buf_fill = len(frame_buffer) / SEQUENCE_LENGTH\n",
    "        buf_color = GREEN if buf_fill >= 1.0 else YELLOW\n",
    "        cv2.putText(frame, f'Buffer: {len(frame_buffer)}/{SEQUENCE_LENGTH}',\n",
    "                    (15, h - 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, buf_color, 1)\n",
    "\n",
    "        # Hand status indicator (shows hand count for two-hand mode)\n",
    "        if NUM_HANDS == 2:\n",
    "            if hands_count == 2:\n",
    "                hand_color = GREEN\n",
    "                hand_text = f'HANDS: 2/2'\n",
    "            elif hands_count == 1:\n",
    "                hand_color = YELLOW\n",
    "                hand_text = f'HANDS: 1/2'\n",
    "            else:\n",
    "                hand_color = RED\n",
    "                hand_text = 'NO HANDS'\n",
    "        else:\n",
    "            hand_color = GREEN if hand_detected else RED\n",
    "            hand_text = 'HAND OK' if hand_detected else 'NO HAND'\n",
    "\n",
    "        cv2.circle(frame, (w - 80, h - 95), 8, hand_color, -1)\n",
    "        cv2.putText(frame, hand_text, (w - 170, h - 90),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, hand_color, 1)\n",
    "\n",
    "        # FPS counter\n",
    "        fps = 1.0 / max(time.time() - frame_start, 1e-6)\n",
    "        fps_history.append(fps)\n",
    "        avg_fps = sum(fps_history) / len(fps_history)\n",
    "        cv2.putText(frame, f'FPS: {avg_fps:.0f}', (w - 110, 115),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, WHITE, 1)\n",
    "\n",
    "        # Mode indicator\n",
    "        mode_text = f'ArSL | {NUM_HANDS}H / {NUM_FEATURES}F'\n",
    "        cv2.putText(frame, mode_text, (w - 220, 135),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (180, 180, 180), 1)\n",
    "\n",
    "        # Cooldown indicator\n",
    "        cooldown_remaining = max(0, COOLDOWN_TIME - (now - last_confirmed_time))\n",
    "        if cooldown_remaining > 0:\n",
    "            cv2.putText(frame, f'Cooldown: {cooldown_remaining:.1f}s',\n",
    "                        (w // 2 - 80, 115), cv2.FONT_HERSHEY_SIMPLEX, 0.6, ORANGE, 2)\n",
    "\n",
    "        # --- Show frame ---\n",
    "        cv2.imshow('ArSL Word Recognition — Live Test', frame)\n",
    "\n",
    "        # --- Handle keyboard ---\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('r'):\n",
    "            sentence_words_en.clear()\n",
    "            sentence_words_ar.clear()\n",
    "            prediction_history.clear()\n",
    "            current_word_en = ''\n",
    "            current_word_ar = ''\n",
    "            print('Sentence reset')\n",
    "        elif key == 32:  # SPACE\n",
    "            sentence_words_en.append(' ')\n",
    "            sentence_words_ar.append(' ')\n",
    "            print('   [space added]')\n",
    "        elif key == 8:   # BACKSPACE\n",
    "            if sentence_words_en:\n",
    "                removed_en = sentence_words_en.pop()\n",
    "                removed_ar = sentence_words_ar.pop() if sentence_words_ar else ''\n",
    "                print(f'Removed: \"{removed_en}\" / \"{removed_ar}\"')\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    final_en = ' '.join(sentence_words_en)\n",
    "    final_ar = ' '.join(sentence_words_ar)\n",
    "    print(f'\\nFinal sentence (EN): {final_en}')\n",
    "    print(f'Final sentence (AR): {final_ar}')\n",
    "    return final_en, final_ar\n",
    "\n",
    "# --- RUN ---\n",
    "result = run_live_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695fdf5",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "| Issue                      | Solution                                                                                       |\n",
    "| -------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| **Low FPS**                | Close other apps, reduce `CAMERA_WIDTH`/`CAMERA_HEIGHT`                                        |\n",
    "| **Wrong predictions**      | Hold the sign steadily for ~2 seconds                                                          |\n",
    "| **Camera not opening**     | Change `CAMERA_INDEX` to 1 or 2                                                                |\n",
    "| **Too sensitive**          | Increase `STABILITY_WINDOW` to 4-5                                                             |\n",
    "| **Not detecting**          | Lower `CONFIDENCE_THRESHOLD` to 0.25                                                           |\n",
    "| **Too slow between words** | Decrease `COOLDOWN_TIME` to 1.0                                                                |\n",
    "| **Only 1 hand shown**      | The model auto-detects hand count from its input shape. Retrain with 2 hands for full support. |\n",
    "\n",
    "### How to perform a sign:\n",
    "\n",
    "1. Face the camera with your hand(s) clearly visible\n",
    "2. Perform the Arabic sign gesture smoothly\n",
    "3. Wait for the stability bar to fill up\n",
    "4. The word will be confirmed and added to the sentence (both English and Arabic)\n",
    "\n",
    "### Two-Hand Mode Notes:\n",
    "\n",
    "- If your model was trained with 126 features (2 hands), both hands will be tracked\n",
    "- Hands are ordered consistently: Left first, Right second\n",
    "- If only one hand is visible, the other hand's landmarks are zero-padded\n",
    "- For best results with two-hand signs, keep both hands in the camera frame\n",
    "\n",
    "### Arabic Display:\n",
    "\n",
    "- The bottom bar shows both **English** and **Arabic** translations\n",
    "- Top-3 predictions show English names (OpenCV has limited Arabic font support)\n",
    "- Full Arabic display requires a GUI framework with Arabic font rendering\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
