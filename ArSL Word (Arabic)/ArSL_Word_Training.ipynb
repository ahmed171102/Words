{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6969240",
   "metadata": {},
   "source": [
    "# Arabic Word Training (KArSL + MediaPipe + BiLSTM)\n",
    "\n",
    "## GPU-Optimized Word-Level Arabic Sign Language Recognition\n",
    "\n",
    "This notebook builds a **word-level** Arabic sign language recognition model using KArSL data filtered by `shared_word_vocabulary.csv`.\n",
    "\n",
    "### Pipeline:\n",
    "\n",
    "1. **GPU Detection & Configuration** ‚Äî memory growth, mixed precision, device verification\n",
    "2. **Config & Imports** ‚Äî all paths and hyper-parameters in one place\n",
    "3. **Load Shared Vocabulary** ‚Äî filter to matched bilingual word set\n",
    "4. **KArSL Data Loading** ‚Äî supports pre-extracted keypoints (.npy/.csv) or raw video (.mp4)\n",
    "5. **Data Exploration** ‚Äî class distribution, sample visualization\n",
    "6. **Preprocessing & Splits** ‚Äî StandardScaler, stratified 60/20/20 split, class weights\n",
    "7. **Build & Train BiLSTM** ‚Äî GPU-accelerated with tf.data pipeline\n",
    "8. **Evaluation** ‚Äî top-1/top-5 accuracy, confusion matrix, classification report, per-category breakdown\n",
    "\n",
    "### Supported Input Formats:\n",
    "\n",
    "- Pre-extracted MediaPipe keypoints (`.npy` or `.csv` files per sample)\n",
    "- Raw video files (`.mp4`) with on-the-fly MediaPipe extraction\n",
    "\n",
    "### Output Artifacts:\n",
    "\n",
    "- `arsl_word_sequences.npz` ‚Äî extracted landmark sequences\n",
    "- `arsl_word_lstm_model_best.h5` ‚Äî best checkpoint (val_accuracy)\n",
    "- `arsl_word_lstm_model_final.h5` ‚Äî final model after early-stopping\n",
    "- `arsl_word_classes.csv` ‚Äî class index ‚Üî word_id mapping\n",
    "\n",
    "### Key: Same Architecture as ASL Word Notebook\n",
    "\n",
    "Both the English and Arabic word models share identical BiLSTM architecture and the same `shared_word_vocabulary.csv`, enabling bilingual translation in the final combined notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c59c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚úÖ All libraries imported successfully!\n",
      "üì¶ TensorFlow : 2.10.0\n",
      "üì¶ NumPy      : 1.23.5\n",
      "üì¶ Pandas     : 2.0.3\n",
      "üì¶ OpenCV     : 4.11.0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 1: Import Required Libraries\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp_lib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚úÖ All libraries imported successfully!')\n",
    "print(f'üì¶ TensorFlow : {tf.__version__}')\n",
    "print(f'üì¶ NumPy      : {np.__version__}')\n",
    "print(f'üì¶ Pandas     : {pd.__version__}')\n",
    "print(f'üì¶ OpenCV     : {cv2.__version__}')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffbf79",
   "metadata": {},
   "source": [
    "## Section 2: GPU Detection and Configuration\n",
    "\n",
    "Automatic GPU detection, memory growth, and optional mixed precision.  \n",
    "**If you get 'Out of Memory' errors** ‚Üí reduce `BATCH_SIZE` to 16 or 8.  \n",
    "**If training shows NaN loss** ‚Üí set `ENABLE_MIXED_PRECISION = False`.  \n",
    "**Monitor GPU** ‚Üí run `nvidia-smi -l 1` in a separate terminal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f767cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç GPU DETECTION & CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "TensorFlow version: 2.10.0\n",
      "Built with CUDA  : True\n",
      "\n",
      "All Physical Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "üéÆ GPU Devices Found: 1\n",
      "\n",
      "‚úÖ GPU IS AVAILABLE!\n",
      "   ‚úÖ Memory growth enabled for 1 GPU(s)\n",
      "   ‚úÖ Using GPU: /physical_device:GPU:0\n",
      "   üìä Device Name       : NVIDIA GeForce MX150\n",
      "   üìä Compute Capability: (6, 1)\n",
      "\n",
      "üìê Using float32 precision (stable for LSTM)\n",
      "\n",
      "üß™ GPU Verification Test...\n",
      "   ‚úÖ GPU computation successful: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "\n",
      "‚úÖ Configuration complete. Using device: /GPU:0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 2: GPU DETECTION & CONFIGURATION\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üîç GPU DETECTION & CONFIGURATION')\n",
    "print('=' * 60)\n",
    "print(f'\\nTensorFlow version: {tf.__version__}')\n",
    "print(f'Built with CUDA  : {tf.test.is_built_with_cuda()}')\n",
    "\n",
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(f'\\nAll Physical Devices: {physical_devices}')\n",
    "\n",
    "# GPU detection\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f'\\nüéÆ GPU Devices Found: {len(gpus)}')\n",
    "\n",
    "USE_GPU = False\n",
    "DEVICE = '/CPU:0'\n",
    "\n",
    "if gpus:\n",
    "    print('\\n‚úÖ GPU IS AVAILABLE!')\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f'   ‚úÖ Memory growth enabled for {len(gpus)} GPU(s)')\n",
    "\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        USE_GPU = True\n",
    "        DEVICE = '/GPU:0'\n",
    "        print(f'   ‚úÖ Using GPU: {gpus[0].name}')\n",
    "\n",
    "        try:\n",
    "            details = tf.config.experimental.get_device_details(gpus[0])\n",
    "            if 'device_name' in details:\n",
    "                print(f'   üìä Device Name       : {details[\"device_name\"]}')\n",
    "            if 'compute_capability' in details:\n",
    "                print(f'   üìä Compute Capability: {details[\"compute_capability\"]}')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f'   ‚ö†Ô∏è  GPU config error: {e}')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è  No GPU detected ‚Äî training on CPU (will be slower)')\n",
    "\n",
    "# Mixed precision ‚Äî safer to keep off for LSTM by default\n",
    "ENABLE_MIXED_PRECISION = False\n",
    "\n",
    "if USE_GPU and ENABLE_MIXED_PRECISION:\n",
    "    try:\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print(f'\\n‚ö° Mixed precision enabled: {policy.name}')\n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ö†Ô∏è  Mixed precision not enabled: {e}')\n",
    "else:\n",
    "    mixed_precision.set_global_policy('float32')\n",
    "    print(f'\\nüìê Using float32 precision (stable for LSTM)')\n",
    "\n",
    "# GPU verification test\n",
    "if USE_GPU:\n",
    "    print('\\nüß™ GPU Verification Test...')\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "        print(f'   ‚úÖ GPU computation successful: {c.device}')\n",
    "    except Exception as e:\n",
    "        print(f'   ‚ùå GPU test failed: {e}')\n",
    "        USE_GPU = False\n",
    "        DEVICE = '/CPU:0'\n",
    "\n",
    "print(f'\\n‚úÖ Configuration complete. Using device: {DEVICE}')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a5d8751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Shared CSV: E:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\SLR Main\\Words\\Shared\\shared_word_vocabulary.csv\n",
      "‚ùå NOT FOUND KArSL root: E:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\SLR Main\\Words\\Datasets\\KArSL_502\n",
      "\n",
      "üìÅ Output dir : E:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\SLR Main\\Words\\ArSL Word (Arabic)\n",
      "\n",
      "‚öôÔ∏è  Sequence length : 30\n",
      "‚öôÔ∏è  Features/frame  : 63\n",
      "‚öôÔ∏è  Batch size      : 32 (auto-scales to 64 on GPU)\n",
      "‚öôÔ∏è  Max epochs      : 100\n",
      "‚öôÔ∏è  Pre-extracted   : True\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 3: Configuration & Paths\n",
    "# ============================================\n",
    "# ‚ö†Ô∏è UPDATE THIS SINGLE PATH for your machine:\n",
    "PROJECT_ROOT = Path(r'E:/Term 9/Grad')\n",
    "\n",
    "# Derived paths\n",
    "SLR_MAIN     = PROJECT_ROOT / 'Main/Sign-Language-Recognition-System-main/SLR Main'\n",
    "WORDS_ROOT   = SLR_MAIN / 'Words'\n",
    "SHARED_CSV   = WORDS_ROOT / 'Shared/shared_word_vocabulary.csv'\n",
    "\n",
    "# ‚ö†Ô∏è SET THIS to your extracted KArSL folder:\n",
    "KARSL_ROOT   = WORDS_ROOT / 'Datasets/KArSL_502'\n",
    "OUTPUT_DIR   = WORDS_ROOT / 'ArSL Word (Arabic)'\n",
    "\n",
    "# ===== HYPER-PARAMETERS =====\n",
    "SEQUENCE_LENGTH = 30       # frames per sample\n",
    "NUM_FEATURES    = 63       # 21 landmarks √ó 3 coords\n",
    "BATCH_SIZE      = 32       # base batch size (CPU fallback)\n",
    "EPOCHS          = 100      # max epochs\n",
    "LEARNING_RATE   = 1e-3     # initial LR\n",
    "LSTM_UNITS_1    = 128      # BiLSTM layer 1\n",
    "LSTM_UNITS_2    = 64       # LSTM layer 2\n",
    "DENSE_UNITS     = 128      # Dense layer before output\n",
    "DROPOUT_RATE    = 0.3\n",
    "TEST_SIZE       = 0.4      # val+test fraction ‚Üí 60/20/20\n",
    "\n",
    "# If True, load pre-extracted .npy/.csv keypoints (faster)\n",
    "# If False, extract from raw .mp4 videos using MediaPipe\n",
    "USE_PREEXTRACTED_KEYPOINTS = True\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify paths\n",
    "for name, path in [('Shared CSV', SHARED_CSV), ('KArSL root', KARSL_ROOT)]:\n",
    "    status = '‚úÖ' if path.exists() else '‚ùå NOT FOUND'\n",
    "    print(f'{status} {name}: {path}')\n",
    "\n",
    "print(f'\\nüìÅ Output dir : {OUTPUT_DIR}')\n",
    "print(f'\\n‚öôÔ∏è  Sequence length : {SEQUENCE_LENGTH}')\n",
    "print(f'‚öôÔ∏è  Features/frame  : {NUM_FEATURES}')\n",
    "print(f'‚öôÔ∏è  Batch size      : {BATCH_SIZE} (auto-scales to 64 on GPU)')\n",
    "print(f'‚öôÔ∏è  Max epochs      : {EPOCHS}')\n",
    "print(f'‚öôÔ∏è  Pre-extracted   : {USE_PREEXTRACTED_KEYPOINTS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151bc988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìö LOADING SHARED VOCABULARY\n",
      "============================================================\n",
      "\n",
      "üìñ Matched vocabulary : 157 Arabic words\n",
      "   Categories         : 9 ‚Äî verb, object, adjective, family, health, direction, job, social, religion\n",
      "   Sample classes     : [71, 83, 88, 90, 92, 104, 113, 114, 115, 116]...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 4: Load Shared Vocabulary\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üìö LOADING SHARED VOCABULARY')\n",
    "print('=' * 60)\n",
    "\n",
    "vocab_df = pd.read_csv(SHARED_CSV)\n",
    "vocab_df = vocab_df.dropna(subset=['karsl_class'])\n",
    "vocab_df['karsl_class'] = vocab_df['karsl_class'].astype(int)\n",
    "\n",
    "karsl_to_wordid  = dict(zip(vocab_df['karsl_class'], vocab_df['word_id'].astype(int)))\n",
    "id_to_english    = dict(zip(vocab_df['word_id'].astype(int), vocab_df['english']))\n",
    "id_to_arabic     = dict(zip(vocab_df['word_id'].astype(int), vocab_df['arabic']))\n",
    "target_karsl_classes = sorted(karsl_to_wordid.keys())\n",
    "\n",
    "print(f'\\nüìñ Matched vocabulary : {len(target_karsl_classes)} Arabic words')\n",
    "print(f'   Categories         : {vocab_df[\"category\"].nunique()} ‚Äî {\", \".join(vocab_df[\"category\"].unique())}')\n",
    "print(f'   Sample classes     : {target_karsl_classes[:10]}...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df407ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 5: Helper Functions\n",
    "# ============================================\n",
    "\n",
    "def pad_or_sample(sequence, target_len=SEQUENCE_LENGTH, target_features=NUM_FEATURES):\n",
    "    \"\"\"Pad (short) or uniformly sample (long) a sequence to fixed shape.\"\"\"\n",
    "    arr = np.array(sequence, dtype=np.float32)\n",
    "    if arr.ndim != 2:\n",
    "        return None\n",
    "\n",
    "    # Adjust feature dimension\n",
    "    if arr.shape[1] > target_features:\n",
    "        arr = arr[:, :target_features]\n",
    "    elif arr.shape[1] < target_features:\n",
    "        pad_feat = np.zeros((arr.shape[0], target_features - arr.shape[1]), dtype=np.float32)\n",
    "        arr = np.concatenate([arr, pad_feat], axis=1)\n",
    "\n",
    "    # Adjust time dimension\n",
    "    if arr.shape[0] >= target_len:\n",
    "        idx = np.linspace(0, arr.shape[0] - 1, target_len, dtype=int)\n",
    "        arr = arr[idx]\n",
    "    else:\n",
    "        pad_time = np.zeros((target_len - arr.shape[0], target_features), dtype=np.float32)\n",
    "        arr = np.concatenate([arr, pad_time], axis=0)\n",
    "\n",
    "    return arr  # shape: (SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "\n",
    "\n",
    "# MediaPipe for video extraction (only used if USE_PREEXTRACTED_KEYPOINTS=False)\n",
    "mp_hands = mp_lib.solutions.hands\n",
    "\n",
    "def extract_from_video(video_path):\n",
    "    \"\"\"Extract MediaPipe hand landmarks from a video file.\"\"\"\n",
    "    hands = mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        hands.close()\n",
    "        return None\n",
    "\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(rgb)\n",
    "\n",
    "        if result.multi_hand_landmarks:\n",
    "            lm = result.multi_hand_landmarks[0]\n",
    "            vec = np.array([[p.x, p.y, p.z] for p in lm.landmark]).flatten()\n",
    "        else:\n",
    "            vec = np.zeros(NUM_FEATURES, dtype=np.float32)\n",
    "        frames.append(vec)\n",
    "\n",
    "    cap.release()\n",
    "    hands.close()\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        return None\n",
    "    return pad_or_sample(np.array(frames, dtype=np.float32))\n",
    "\n",
    "print('‚úÖ Helper functions defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1fc4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì¶ BUILDING ARABIC WORD DATASET\n",
      "============================================================\n",
      "\n",
      "‚ùå KArSL dataset NOT FOUND at: E:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\SLR Main\\Words\\Datasets\\KArSL_502\n",
      "   Please download KArSL-502 from Kaggle:\n",
      "   https://www.kaggle.com/datasets/yousefelkilany/karsl-502\n",
      "   Extract it to: E:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\SLR Main\\Words\\Datasets\\KArSL_502\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "KArSL dataset not found: E:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\SLR Main\\Words\\Datasets\\KArSL_502",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   https://www.kaggle.com/datasets/yousefelkilany/karsl-502\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   Extract it to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mKARSL_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKArSL dataset not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mKARSL_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚è≥ Loading KArSL data from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mKARSL_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: KArSL dataset not found: E:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\SLR Main\\Words\\Datasets\\KArSL_502"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 6: Build Dataset (or Load Cached)\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üì¶ BUILDING ARABIC WORD DATASET')\n",
    "print('=' * 60)\n",
    "\n",
    "NPZ_PATH = OUTPUT_DIR / 'arsl_word_sequences.npz'\n",
    "\n",
    "if NPZ_PATH.exists():\n",
    "    print(f'\\nüíæ Cached data found: {NPZ_PATH}')\n",
    "    data = np.load(NPZ_PATH)\n",
    "    X, y = data['X'], data['y']\n",
    "    print(f'   X shape : {X.shape}')\n",
    "    print(f'   y shape : {y.shape}')\n",
    "    print(f'   Classes : {len(np.unique(y))}')\n",
    "    print('   ‚úÖ Loaded from cache ‚Äî skipping extraction')\n",
    "else:\n",
    "    if not KARSL_ROOT.exists():\n",
    "        print(f'\\n‚ùå KArSL dataset NOT FOUND at: {KARSL_ROOT}')\n",
    "        print('   Please download KArSL-502 from Kaggle:')\n",
    "        print('   https://www.kaggle.com/datasets/yousefelkilany/karsl-502')\n",
    "        print(f'   Extract it to: {KARSL_ROOT}')\n",
    "        raise FileNotFoundError(f'KArSL dataset not found: {KARSL_ROOT}')\n",
    "\n",
    "    print(f'\\n‚è≥ Loading KArSL data from: {KARSL_ROOT}')\n",
    "    start_time = time.time()\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    found_classes, empty_classes = 0, 0\n",
    "\n",
    "    for karsl_class in tqdm(target_karsl_classes, desc='Loading KArSL classes'):\n",
    "        word_id = int(karsl_to_wordid[karsl_class])\n",
    "\n",
    "        # Try common folder naming conventions\n",
    "        candidates = [\n",
    "            KARSL_ROOT / str(karsl_class),\n",
    "            KARSL_ROOT / f'{karsl_class:03d}',\n",
    "            KARSL_ROOT / f'{karsl_class:04d}',\n",
    "            KARSL_ROOT / f'class_{karsl_class}',\n",
    "        ]\n",
    "        class_dir = next((p for p in candidates if p.exists()), None)\n",
    "        if class_dir is None:\n",
    "            empty_classes += 1\n",
    "            continue\n",
    "\n",
    "        found_classes += 1\n",
    "\n",
    "        # Collect all data files\n",
    "        if USE_PREEXTRACTED_KEYPOINTS:\n",
    "            files = list(class_dir.rglob('*.npy')) + list(class_dir.rglob('*.csv'))\n",
    "        else:\n",
    "            files = list(class_dir.rglob('*.mp4'))\n",
    "\n",
    "        if not files:\n",
    "            # Fallback: try all types\n",
    "            files = list(class_dir.rglob('*.npy')) + list(class_dir.rglob('*.csv')) + list(class_dir.rglob('*.mp4'))\n",
    "\n",
    "        for fp in files:\n",
    "            seq = None\n",
    "            try:\n",
    "                if fp.suffix.lower() == '.npy':\n",
    "                    arr = np.load(fp)\n",
    "                    seq = pad_or_sample(arr)\n",
    "                elif fp.suffix.lower() == '.csv':\n",
    "                    arr = pd.read_csv(fp).values\n",
    "                    seq = pad_or_sample(arr)\n",
    "                elif fp.suffix.lower() == '.mp4':\n",
    "                    seq = extract_from_video(fp)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if seq is None:\n",
    "                continue\n",
    "\n",
    "            # Skip blank sequences (<20% hand detection)\n",
    "            blank_ratio = np.sum(np.all(seq == 0, axis=1)) / len(seq)\n",
    "            if blank_ratio > 0.8:\n",
    "                continue\n",
    "\n",
    "            X_list.append(seq)\n",
    "            y_list.append(word_id)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.int32)\n",
    "\n",
    "    print(f'\\n‚úÖ Dataset built in {elapsed:.1f}s ({elapsed/60:.1f} min)')\n",
    "    print(f'   X shape       : {X.shape}')\n",
    "    print(f'   y shape       : {y.shape}')\n",
    "    print(f'   Classes found : {found_classes} / {len(target_karsl_classes)}')\n",
    "    print(f'   Empty classes  : {empty_classes}')\n",
    "\n",
    "    np.savez_compressed(NPZ_PATH, X=X, y=y)\n",
    "    print(f'\\nüíæ Saved: {NPZ_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 7: Data Exploration\n",
    "# ÿßÿ≥ÿ™ŸÉÿ¥ÿßŸÅ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üìä DATA EXPLORATION / ÿßÿ≥ÿ™ŸÉÿ¥ÿßŸÅ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™')\n",
    "print('=' * 60)\n",
    "\n",
    "unique_ids, counts = np.unique(y, return_counts=True)\n",
    "word_names_en = [id_to_english.get(int(uid), str(uid)) for uid in unique_ids]\n",
    "word_names_ar = [id_to_arabic.get(int(uid), str(uid)) for uid in unique_ids]\n",
    "\n",
    "# Sort by count descending\n",
    "sort_idx = np.argsort(counts)[::-1]\n",
    "sorted_names  = [f'{word_names_en[i]} / {word_names_ar[i]}' for i in sort_idx]\n",
    "sorted_counts = counts[sort_idx]\n",
    "\n",
    "# =============================================\n",
    "# PLOT 1: Class distribution\n",
    "# =============================================\n",
    "fig, ax = plt.subplots(figsize=(24, 7))\n",
    "ax.bar(range(len(sorted_names)), sorted_counts, color='darkgreen', edgecolor='black', linewidth=0.3)\n",
    "ax.set_xticks(range(len(sorted_names)))\n",
    "ax.set_xticklabels(sorted_names, rotation=90, fontsize=5)\n",
    "ax.set_xlabel('Word (English / Arabic)', fontsize=12)\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.set_title(f'Arabic Word Dataset Distribution ‚Äî {len(unique_ids)} classes, {len(y)} total samples', fontsize=14)\n",
    "ax.axhline(y=np.mean(sorted_counts), color='red', linestyle='--', alpha=0.7, label=f'Mean: {np.mean(sorted_counts):.1f}')\n",
    "ax.axhline(y=np.median(sorted_counts), color='orange', linestyle=':', alpha=0.7, label=f'Median: {np.median(sorted_counts):.1f}')\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüìä Dataset Summary:')\n",
    "print(f'   Total samples    : {len(y)}')\n",
    "print(f'   Total classes    : {len(unique_ids)}')\n",
    "print(f'   Min samples/class: {counts.min()} ({word_names_en[counts.argmin()]})')\n",
    "print(f'   Max samples/class: {counts.max()} ({word_names_en[counts.argmax()]})')\n",
    "print(f'   Mean             : {counts.mean():.1f}')\n",
    "print(f'   Std              : {counts.std():.1f}')\n",
    "print(f'   Median           : {np.median(counts):.1f}')\n",
    "\n",
    "low_sample = [(word_names_en[i], counts[i]) for i in range(len(counts)) if counts[i] < 5]\n",
    "if low_sample:\n",
    "    print(f'\\n‚ö†Ô∏è  Classes with <5 samples ({len(low_sample)}):')\n",
    "    for name, cnt in low_sample:\n",
    "        print(f'   {name}: {cnt}')\n",
    "\n",
    "# =============================================\n",
    "# PLOT 2: Class frequency histogram + Zero-frame quality + Feature distribution\n",
    "# =============================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 5))\n",
    "\n",
    "# 2a: Histogram of samples-per-class\n",
    "axes[0].hist(sorted_counts, bins=20, color='darkgreen', edgecolor='black', alpha=0.85)\n",
    "axes[0].set_xlabel('Samples per Class', fontsize=11)\n",
    "axes[0].set_ylabel('Number of Classes', fontsize=11)\n",
    "axes[0].set_title('How Many Classes Have N Samples?', fontsize=13)\n",
    "axes[0].axvline(x=np.mean(sorted_counts), color='red', linestyle='--', label=f'Mean: {np.mean(sorted_counts):.1f}')\n",
    "axes[0].axvline(x=np.median(sorted_counts), color='orange', linestyle=':', label=f'Median: {np.median(sorted_counts):.1f}')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# 2b: Zero-frame quality analysis\n",
    "zero_ratios = []\n",
    "for i in range(len(X)):\n",
    "    frame_sums = np.sum(np.abs(X[i]), axis=1)\n",
    "    zero_frames = np.sum(frame_sums == 0)\n",
    "    zero_ratios.append(zero_frames / SEQUENCE_LENGTH * 100)\n",
    "zero_ratios = np.array(zero_ratios)\n",
    "\n",
    "axes[1].hist(zero_ratios, bins=30, color='coral', edgecolor='darkred', alpha=0.85)\n",
    "axes[1].set_xlabel('% Zero Frames per Sample', fontsize=11)\n",
    "axes[1].set_ylabel('Number of Samples', fontsize=11)\n",
    "axes[1].set_title(f'Sequence Quality ‚Äî Mean: {np.mean(zero_ratios):.1f}% zero frames', fontsize=13)\n",
    "axes[1].axvline(x=np.mean(zero_ratios), color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 2c: Feature value distribution (boxplot of x-coordinates per landmark)\n",
    "subsample = X[:min(200, len(X))].reshape(-1, NUM_FEATURES)\n",
    "landmark_x = [subsample[:, i*3] for i in range(21)]\n",
    "bp = axes[2].boxplot(landmark_x, whis=1.5, showfliers=False, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='#C8E6C9', edgecolor='darkgreen'))\n",
    "axes[2].set_xlabel('Landmark Index', fontsize=11)\n",
    "axes[2].set_ylabel('X-Coordinate Value', fontsize=11)\n",
    "axes[2].set_title('Landmark X-Coordinate Distribution (first 200 samples)', fontsize=13)\n",
    "axes[2].set_xticklabels([f'{i}' for i in range(21)], fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüîç Quality Stats:')\n",
    "print(f'   Samples with >50% zero frames: {np.sum(zero_ratios > 50)} ({np.sum(zero_ratios > 50)/len(zero_ratios)*100:.1f}%)')\n",
    "print(f'   Samples with 0% zero frames  : {np.sum(zero_ratios == 0)} ({np.sum(zero_ratios == 0)/len(zero_ratios)*100:.1f}%)')\n",
    "\n",
    "# =============================================\n",
    "# PLOT 3: 2D Hand Landmark Trajectories\n",
    "# =============================================\n",
    "sample_idx = 0\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "sample = X[sample_idx]\n",
    "colors = np.arange(SEQUENCE_LENGTH)\n",
    "\n",
    "# Wrist trajectory (landmark 0)\n",
    "sc0 = axes[0].scatter(sample[:, 0], sample[:, 1], c=colors, cmap='viridis', s=25, zorder=2)\n",
    "axes[0].plot(sample[:, 0], sample[:, 1], 'k-', alpha=0.2, linewidth=0.5, zorder=1)\n",
    "axes[0].set_xlabel('X', fontsize=11)\n",
    "axes[0].set_ylabel('Y', fontsize=11)\n",
    "axes[0].set_title('Wrist (L0) Trajectory', fontsize=13)\n",
    "axes[0].invert_yaxis()\n",
    "plt.colorbar(sc0, ax=axes[0], label='Frame')\n",
    "\n",
    "# Index fingertip trajectory (landmark 8: index 24,25)\n",
    "sc1 = axes[1].scatter(sample[:, 24], sample[:, 25], c=colors, cmap='plasma', s=25, zorder=2)\n",
    "axes[1].plot(sample[:, 24], sample[:, 25], 'k-', alpha=0.2, linewidth=0.5, zorder=1)\n",
    "axes[1].set_xlabel('X', fontsize=11)\n",
    "axes[1].set_ylabel('Y', fontsize=11)\n",
    "axes[1].set_title('Index Fingertip (L8) Trajectory', fontsize=13)\n",
    "axes[1].invert_yaxis()\n",
    "plt.colorbar(sc1, ax=axes[1], label='Frame')\n",
    "\n",
    "# Middle fingertip trajectory (landmark 12: index 36,37)\n",
    "sc2 = axes[2].scatter(sample[:, 36], sample[:, 37], c=colors, cmap='coolwarm', s=25, zorder=2)\n",
    "axes[2].plot(sample[:, 36], sample[:, 37], 'k-', alpha=0.2, linewidth=0.5, zorder=1)\n",
    "axes[2].set_xlabel('X', fontsize=11)\n",
    "axes[2].set_ylabel('Y', fontsize=11)\n",
    "axes[2].set_title('Middle Fingertip (L12) Trajectory', fontsize=13)\n",
    "axes[2].invert_yaxis()\n",
    "plt.colorbar(sc2, ax=axes[2], label='Frame')\n",
    "\n",
    "word_en = id_to_english.get(int(y[sample_idx]), '?')\n",
    "word_ar = id_to_arabic.get(int(y[sample_idx]), '?')\n",
    "plt.suptitle(f'Hand Landmark Trajectories ‚Äî \"{word_en}\" / \"{word_ar}\" (color = time)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 4: Landmark Heatmap of sample sequence\n",
    "# =============================================\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.imshow(X[sample_idx].T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Coordinate value')\n",
    "plt.xlabel('Frame (time step)', fontsize=12)\n",
    "plt.ylabel('Feature index (landmark √ó coord)', fontsize=12)\n",
    "plt.title(f'Landmark Heatmap: \"{word_en}\" / \"{word_ar}\" ‚Äî shape {X[sample_idx].shape}', fontsize=14)\n",
    "\n",
    "# Add landmark boundary lines\n",
    "for lm in range(1, 21):\n",
    "    plt.axhline(y=lm*3 - 0.5, color='white', linewidth=0.3, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eda638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 8: Preprocessing & Splits\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üîß PREPROCESSING & TRAIN/VAL/TEST SPLIT')\n",
    "print('=' * 60)\n",
    "\n",
    "# Reload from cache\n",
    "data = np.load(NPZ_PATH)\n",
    "X, y = data['X'], data['y']\n",
    "\n",
    "# StandardScaler normalization\n",
    "original_shape = X.shape\n",
    "X_flat = X.reshape(-1, NUM_FEATURES)\n",
    "scaler = StandardScaler()\n",
    "X_flat = scaler.fit_transform(X_flat)\n",
    "X = X_flat.reshape(original_shape).astype(np.float32)\n",
    "print(f'   ‚úÖ StandardScaler applied: mean‚âà0, std‚âà1 per feature')\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "num_classes = len(encoder.classes_)\n",
    "y_onehot = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Stratified split: 60/20/20\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_onehot, test_size=TEST_SIZE, random_state=42, stratify=y_encoded\n",
    ")\n",
    "temp_labels = np.argmax(y_temp, axis=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "# Class weights for imbalanced data\n",
    "train_labels = np.argmax(y_train, axis=1)\n",
    "cw = compute_class_weight('balanced', classes=np.arange(num_classes), y=train_labels)\n",
    "class_weights = dict(enumerate(cw))\n",
    "\n",
    "print(f'\\nüìä Split Summary:')\n",
    "print(f'   Classes      : {num_classes}')\n",
    "print(f'   Train        : {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.0f}%)')\n",
    "print(f'   Validation   : {X_val.shape[0]} ({X_val.shape[0]/len(X)*100:.0f}%)')\n",
    "print(f'   Test         : {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.0f}%)')\n",
    "print(f'   Input shape  : {X_train.shape[1:]}')\n",
    "print(f'   Class weights: balanced ‚Äî max: {max(cw):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 9: Build & Train BiLSTM (GPU-Optimized)\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üöÄ BUILDING & TRAINING BiLSTM MODEL')\n",
    "print('=' * 60)\n",
    "\n",
    "# Clear previous session for clean GPU memory state\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Adaptive batch size: GPU uses larger batches for throughput\n",
    "BATCH_SIZE_TRAIN = 64 if USE_GPU else BATCH_SIZE\n",
    "print(f'   üì¶ Batch size (auto): {BATCH_SIZE_TRAIN} ({\"GPU\" if USE_GPU else \"CPU\"})')\n",
    "\n",
    "# --- tf.data pipeline ---\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.shuffle(buffer_size=min(len(X_train), 10000), seed=42, reshuffle_each_iteration=True)\n",
    "train_ds = train_ds.batch(BATCH_SIZE_TRAIN).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE_TRAIN).prefetch(AUTOTUNE)\n",
    "\n",
    "print(f'   ‚úÖ tf.data pipelines created (shuffle + batch + prefetch)')\n",
    "\n",
    "# --- Build model (identical architecture to ASL Word notebook) ---\n",
    "# ‚ö° IMPORTANT: recurrent_dropout is intentionally NOT used.\n",
    "# When recurrent_dropout=0, TensorFlow uses NVIDIA cuDNN LSTM kernels\n",
    "# which are 5-10x faster on GPU. Regular Dropout layers provide\n",
    "# the same regularization effect.\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    model = Sequential([\n",
    "        Input(shape=(SEQUENCE_LENGTH, NUM_FEATURES), name='landmark_sequence'),\n",
    "\n",
    "        # BiLSTM block 1 ‚Äî reads forward + backward (cuDNN-accelerated)\n",
    "        Bidirectional(LSTM(LSTM_UNITS_1, return_sequences=True), name='bilstm_1'),\n",
    "        BatchNormalization(name='bn_1'),\n",
    "        Dropout(DROPOUT_RATE, name='drop_1'),\n",
    "\n",
    "        # LSTM block 2 ‚Äî outputs final hidden state (cuDNN-accelerated)\n",
    "        LSTM(LSTM_UNITS_2, name='lstm_2'),\n",
    "        BatchNormalization(name='bn_2'),\n",
    "        Dropout(DROPOUT_RATE, name='drop_2'),\n",
    "\n",
    "        # Dense classifier (with L2 regularization + He initialization)\n",
    "        Dense(DENSE_UNITS, activation='relu',\n",
    "              kernel_initializer='he_normal',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "              name='dense_1'),\n",
    "        Dropout(DROPOUT_RATE - 0.1, name='drop_3'),\n",
    "        Dense(num_classes, activation='softmax', dtype='float32', name='output')\n",
    "    ], name='ArSL_Word_BiLSTM')\n",
    "\n",
    "# Use legacy Adam for better GPU + mixed precision compatibility\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('\\nüìê Model Architecture:')\n",
    "model.summary()\n",
    "print(f'\\nüñ•Ô∏è  Model will train on: {DEVICE}')\n",
    "\n",
    "# --- Callbacks ---\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        str(OUTPUT_DIR / 'arsl_word_lstm_model_best.h5'),\n",
    "        monitor='val_accuracy', save_best_only=True, mode='max', verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# --- Train (GPU-accelerated) ---\n",
    "print(f'\\nüöÄ Starting training...')\n",
    "print(f'   Device       : {DEVICE}')\n",
    "print(f'   Batch size   : {BATCH_SIZE_TRAIN}')\n",
    "print(f'   Max epochs   : {EPOCHS}')\n",
    "print(f'   LR           : {LEARNING_RATE}')\n",
    "print(f'   cuDNN LSTM   : ‚úÖ enabled (no recurrent_dropout)')\n",
    "print(f'   Class weights: ‚úÖ enabled (balanced)')\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f'\\n‚úÖ Training complete in {training_time:.1f}s ({training_time/60:.1f} min)')\n",
    "print(f'   Best val_accuracy: {max(history.history[\"val_accuracy\"]):.4f}')\n",
    "print(f'   Final LR         : {model.optimizer.learning_rate.numpy():.2e}')\n",
    "\n",
    "# Save\n",
    "model.save(str(OUTPUT_DIR / 'arsl_word_lstm_model_final.h5'))\n",
    "class_df = pd.DataFrame({\n",
    "    'model_class_index': range(num_classes),\n",
    "    'word_id': encoder.classes_.tolist()\n",
    "})\n",
    "class_df.to_csv(OUTPUT_DIR / 'arsl_word_classes.csv', index=False)\n",
    "print(f'\\nüíæ Final model : {OUTPUT_DIR / \"arsl_word_lstm_model_final.h5\"}')\n",
    "print(f'üíæ Best model  : {OUTPUT_DIR / \"arsl_word_lstm_model_best.h5\"}')\n",
    "print(f'üíæ Class map   : {OUTPUT_DIR / \"arsl_word_classes.csv\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6610e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 10: Evaluation & Visualization Dashboard\n",
    "# ÿßŸÑÿ™ŸÇŸäŸäŸÖ ŸàŸÑŸàÿ≠ÿ© ÿßŸÑÿ™ÿµŸàÿ± ÿßŸÑÿ®ÿµÿ±Ÿä\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('üìà MODEL EVALUATION & VISUALIZATION DASHBOARD')\n",
    "print('=' * 60)\n",
    "\n",
    "# Load best checkpoint\n",
    "best_model = tf.keras.models.load_model(str(OUTPUT_DIR / 'arsl_word_lstm_model_best.h5'))\n",
    "\n",
    "# Predict using optimized pipeline\n",
    "eval_batch = 64 if USE_GPU else 32\n",
    "eval_ds = tf.data.Dataset.from_tensor_slices((X_test,)).batch(eval_batch).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    proba = best_model.predict(eval_ds, verbose=0)\n",
    "\n",
    "y_pred = np.argmax(proba, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Top-1 accuracy\n",
    "top1_acc = (y_pred == y_true).mean()\n",
    "\n",
    "# Top-5 accuracy\n",
    "top5_correct = 0\n",
    "for i in range(len(y_true)):\n",
    "    top5 = np.argsort(proba[i])[-5:]\n",
    "    if y_true[i] in top5:\n",
    "        top5_correct += 1\n",
    "top5_acc = top5_correct / len(y_true)\n",
    "\n",
    "print(f'\\nüéØ Test Results:')\n",
    "print(f'   Top-1 Accuracy : {top1_acc:.4f} ({top1_acc*100:.2f}%)')\n",
    "print(f'   Top-5 Accuracy : {top5_acc:.4f} ({top5_acc*100:.2f}%)')\n",
    "print(f'   Test samples   : {len(y_true)}')\n",
    "print(f'   Classes         : {num_classes}')\n",
    "\n",
    "# =============================================\n",
    "# PLOT 1: Training Dashboard (4 panels)\n",
    "# =============================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1a: Accuracy curves\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2, color='#2E7D32')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2, color='#FF9800')\n",
    "axes[0, 0].fill_between(range(len(history.history['accuracy'])),\n",
    "                         history.history['accuracy'], history.history['val_accuracy'],\n",
    "                         alpha=0.1, color='red')\n",
    "axes[0, 0].set_title('Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylim([0, 1.05])\n",
    "best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "axes[0, 0].axvline(x=best_epoch, color='blue', linestyle=':', alpha=0.5, label=f'Best: epoch {best_epoch}')\n",
    "\n",
    "# 1b: Loss curves\n",
    "axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2, color='#2E7D32')\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2, color='#FF9800')\n",
    "axes[0, 1].fill_between(range(len(history.history['loss'])),\n",
    "                         history.history['loss'], history.history['val_loss'],\n",
    "                         alpha=0.1, color='red')\n",
    "axes[0, 1].set_title('Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 1c: Learning Rate schedule\n",
    "if 'lr' in history.history:\n",
    "    lr_values = history.history['lr']\n",
    "else:\n",
    "    lr_values = [LEARNING_RATE] * len(history.history['loss'])\n",
    "axes[1, 0].plot(lr_values, linewidth=2, color='#4CAF50', marker='o', markersize=3)\n",
    "axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 1d: Overfitting gap (train_acc - val_acc)\n",
    "train_acc = np.array(history.history['accuracy'])\n",
    "val_acc = np.array(history.history['val_accuracy'])\n",
    "gap = train_acc - val_acc\n",
    "axes[1, 1].bar(range(len(gap)), gap, color=['green' if g < 0.05 else 'orange' if g < 0.15 else 'red' for g in gap],\n",
    "               edgecolor='black', linewidth=0.3, alpha=0.8)\n",
    "axes[1, 1].axhline(y=0.05, color='green', linestyle='--', alpha=0.5, label='Healthy gap (5%)')\n",
    "axes[1, 1].axhline(y=0.15, color='red', linestyle='--', alpha=0.5, label='Overfitting threshold (15%)')\n",
    "axes[1, 1].set_title('Overfitting Monitor (Train - Val Accuracy)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy Gap')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Arabic Word BiLSTM Training Dashboard ‚Äî Top-1: {top1_acc*100:.1f}%, Top-5: {top5_acc*100:.1f}%',\n",
    "             fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 2: Prediction Confidence Distribution\n",
    "# =============================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "correct_mask = y_pred == y_true\n",
    "correct_conf = np.max(proba[correct_mask], axis=1)\n",
    "wrong_conf   = np.max(proba[~correct_mask], axis=1) if np.sum(~correct_mask) > 0 else np.array([])\n",
    "\n",
    "axes[0].hist(correct_conf, bins=30, alpha=0.7, color='#4CAF50', edgecolor='darkgreen', label=f'Correct ({len(correct_conf)})')\n",
    "if len(wrong_conf) > 0:\n",
    "    axes[0].hist(wrong_conf, bins=30, alpha=0.7, color='#F44336', edgecolor='darkred', label=f'Wrong ({len(wrong_conf)})')\n",
    "axes[0].set_xlabel('Prediction Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Confidence Distribution: Correct vs Wrong', fontsize=13)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Confidence margin (top-1 vs top-2)\n",
    "top1_conf = np.max(proba, axis=1)\n",
    "sorted_proba = np.sort(proba, axis=1)[:, ::-1]\n",
    "top2_conf = sorted_proba[:, 1] if proba.shape[1] > 1 else np.zeros(len(proba))\n",
    "margin = top1_conf - top2_conf\n",
    "\n",
    "axes[1].hist(margin, bins=30, color='#9C27B0', edgecolor='purple', alpha=0.8)\n",
    "axes[1].set_xlabel('Confidence Margin (Top-1 ‚àí Top-2)', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title(f'Decision Margin ‚Äî Mean: {np.mean(margin):.3f}', fontsize=13)\n",
    "axes[1].axvline(x=np.mean(margin), color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 3: Classification Report with bilingual labels\n",
    "# =============================================\n",
    "word_labels = []\n",
    "for cls_idx in range(num_classes):\n",
    "    wid = int(encoder.classes_[cls_idx])\n",
    "    en = id_to_english.get(wid, str(wid))\n",
    "    ar = id_to_arabic.get(wid, '')\n",
    "    word_labels.append(f'{en}/{ar}')\n",
    "\n",
    "# Short labels for bar charts (English only)\n",
    "word_labels_short = []\n",
    "for cls_idx in range(num_classes):\n",
    "    wid = int(encoder.classes_[cls_idx])\n",
    "    word_labels_short.append(id_to_english.get(wid, str(wid)))\n",
    "\n",
    "print('\\nüìã Classification Report:')\n",
    "report = classification_report(y_true, y_pred, target_names=word_labels, zero_division=0, output_dict=True)\n",
    "print(classification_report(y_true, y_pred, target_names=word_labels, zero_division=0))\n",
    "\n",
    "# =============================================\n",
    "# PLOT 4: Per-Class F1 Score Bar Chart\n",
    "# =============================================\n",
    "class_f1 = {k: v['f1-score'] for k, v in report.items() if k in word_labels}\n",
    "sorted_f1 = sorted(class_f1.items(), key=lambda x: x[1], reverse=True)\n",
    "f1_names = [x[0] for x in sorted_f1]\n",
    "f1_vals  = [x[1] for x in sorted_f1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 6))\n",
    "colors_f1 = ['#4CAF50' if v >= 0.7 else '#FF9800' if v >= 0.4 else '#F44336' for v in f1_vals]\n",
    "ax.bar(range(len(f1_names)), f1_vals, color=colors_f1, edgecolor='black', linewidth=0.3)\n",
    "ax.set_xticks(range(len(f1_names)))\n",
    "ax.set_xticklabels(f1_names, rotation=90, fontsize=5)\n",
    "ax.set_xlabel('Word', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title(f'Per-Class F1 Score (green ‚â•0.7, orange ‚â•0.4, red <0.4) ‚Äî Mean: {np.mean(f1_vals):.3f}', fontsize=14)\n",
    "ax.axhline(y=np.mean(f1_vals), color='blue', linestyle='--', alpha=0.5, label=f'Mean F1: {np.mean(f1_vals):.3f}')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 5: Confusion Matrix (enhanced)\n",
    "# =============================================\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 18))\n",
    "if num_classes <= 50:\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "                xticklabels=word_labels, yticklabels=word_labels, ax=ax,\n",
    "                linewidths=0.5, linecolor='lightgray')\n",
    "else:\n",
    "    sns.heatmap(cm, annot=False, cmap='Greens',\n",
    "                xticklabels=word_labels, yticklabels=word_labels, ax=ax)\n",
    "ax.set_title(f'Arabic Confusion Matrix ‚Äî {num_classes} classes (Top-1: {top1_acc*100:.1f}%)', fontsize=15)\n",
    "ax.set_xlabel('Predicted', fontsize=13)\n",
    "ax.set_ylabel('True', fontsize=13)\n",
    "plt.xticks(rotation=90, fontsize=5)\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 6: Top-10 Most Confused Pairs\n",
    "# =============================================\n",
    "np.fill_diagonal(cm, 0)\n",
    "confused_pairs = []\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        if cm[i, j] > 0:\n",
    "            confused_pairs.append((word_labels_short[i], word_labels_short[j], cm[i, j]))\n",
    "confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "top_confused = confused_pairs[:10]\n",
    "\n",
    "if top_confused:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    pair_labels = [f'{p[0]} ‚Üí {p[1]}' for p in top_confused]\n",
    "    pair_counts = [p[2] for p in top_confused]\n",
    "    bars = ax.barh(range(len(pair_labels)), pair_counts, color='#E91E63', edgecolor='darkred', alpha=0.85)\n",
    "    ax.set_yticks(range(len(pair_labels)))\n",
    "    ax.set_yticklabels(pair_labels, fontsize=10)\n",
    "    ax.set_xlabel('Misclassification Count', fontsize=12)\n",
    "    ax.set_title('Top-10 Most Confused Pairs (True ‚Üí Predicted)', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    for bar, count in zip(bars, pair_counts):\n",
    "        ax.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,\n",
    "                str(count), va='center', fontsize=10, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 7: Per-Category Accuracy (bar chart)\n",
    "# =============================================\n",
    "cat_map = dict(zip(vocab_df['word_id'].astype(int), vocab_df['category']))\n",
    "category_correct, category_total = {}, {}\n",
    "for i in range(len(y_true)):\n",
    "    wid = int(encoder.classes_[y_true[i]])\n",
    "    cat = cat_map.get(wid, 'unknown')\n",
    "    category_total[cat] = category_total.get(cat, 0) + 1\n",
    "    if y_pred[i] == y_true[i]:\n",
    "        category_correct[cat] = category_correct.get(cat, 0) + 1\n",
    "\n",
    "cat_names = sorted(category_total.keys())\n",
    "cat_accs  = [category_correct.get(c, 0) / category_total[c] for c in cat_names]\n",
    "cat_sizes = [category_total[c] for c in cat_names]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x_pos = range(len(cat_names))\n",
    "bars = ax1.bar(x_pos, [a * 100 for a in cat_accs], color='#2E7D32', edgecolor='black', alpha=0.85, label='Accuracy %')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(cat_names, rotation=45, ha='right', fontsize=11)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_ylim([0, 105])\n",
    "\n",
    "for bar, acc, size in zip(bars, cat_accs, cat_sizes):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "             f'{acc*100:.1f}%\\n(n={size})', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax1.set_title('Per-Category Accuracy with Sample Counts', fontsize=14, fontweight='bold')\n",
    "ax1.axhline(y=top1_acc*100, color='red', linestyle='--', alpha=0.5, label=f'Overall: {top1_acc*100:.1f}%')\n",
    "ax1.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 8: Best & Worst Performing Classes\n",
    "# =============================================\n",
    "per_class_acc = {}\n",
    "for i in range(num_classes):\n",
    "    mask = y_true == i\n",
    "    if mask.sum() > 0:\n",
    "        per_class_acc[word_labels_short[i]] = (y_pred[mask] == i).mean()\n",
    "\n",
    "sorted_acc = sorted(per_class_acc.items(), key=lambda x: x[1])\n",
    "n_show = min(10, len(sorted_acc))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Worst\n",
    "worst = sorted_acc[:n_show]\n",
    "ax1.barh(range(len(worst)), [w[1]*100 for w in worst], color='#F44336', edgecolor='darkred', alpha=0.85)\n",
    "ax1.set_yticks(range(len(worst)))\n",
    "ax1.set_yticklabels([w[0] for w in worst], fontsize=10)\n",
    "ax1.set_xlabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title(f'Bottom {n_show} Performing Classes', fontsize=14, fontweight='bold', color='#F44336')\n",
    "ax1.set_xlim([0, 105])\n",
    "for i, w in enumerate(worst):\n",
    "    ax1.text(w[1]*100 + 1, i, f'{w[1]*100:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "# Best\n",
    "best = sorted_acc[-n_show:][::-1]\n",
    "ax2.barh(range(len(best)), [b[1]*100 for b in best], color='#4CAF50', edgecolor='darkgreen', alpha=0.85)\n",
    "ax2.set_yticks(range(len(best)))\n",
    "ax2.set_yticklabels([b[0] for b in best], fontsize=10)\n",
    "ax2.set_xlabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title(f'Top {n_show} Performing Classes', fontsize=14, fontweight='bold', color='#4CAF50')\n",
    "ax2.set_xlim([0, 105])\n",
    "for i, b in enumerate(best):\n",
    "    ax2.text(b[1]*100 + 1, i, f'{b[1]*100:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('Best vs Worst Performing Classes', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# PLOT 9: Precision vs Recall Scatter\n",
    "# =============================================\n",
    "precisions = [report[w]['precision'] for w in word_labels if w in report]\n",
    "recalls = [report[w]['recall'] for w in word_labels if w in report]\n",
    "f1s = [report[w]['f1-score'] for w in word_labels if w in report]\n",
    "labels_in_report = [w for w in word_labels if w in report]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(recalls, precisions, c=f1s, cmap='RdYlGn', s=50, edgecolors='black', linewidth=0.5, alpha=0.8)\n",
    "plt.colorbar(scatter, label='F1 Score', ax=ax)\n",
    "ax.set_xlabel('Recall', fontsize=13)\n",
    "ax.set_ylabel('Precision', fontsize=13)\n",
    "ax.set_title('Precision vs Recall per Class (color = F1)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([-0.05, 1.05])\n",
    "ax.set_ylim([-0.05, 1.05])\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate worst classes\n",
    "for i, lbl in enumerate(labels_in_report):\n",
    "    if f1s[i] < 0.3:\n",
    "        ax.annotate(lbl, (recalls[i], precisions[i]), fontsize=6, alpha=0.8,\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('‚úÖ Evaluation & Visualization Dashboard complete!')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99506c30",
   "metadata": {},
   "source": [
    "## Tips & Troubleshooting / ŸÜÿµÿßÿ¶ÿ≠ Ÿàÿ≠ŸÑ ÿßŸÑŸÖÿ¥ÿßŸÉŸÑ\n",
    "\n",
    "| Issue / ÿßŸÑŸÖÿ¥ŸÉŸÑÿ©            | Solution / ÿßŸÑÿ≠ŸÑ                                                              |\n",
    "| -------------------------- | ---------------------------------------------------------------------------- |\n",
    "| **OOM (Out of Memory)**    | Reduce `BATCH_SIZE` to 64 or 32 / ŸÇŸÑŸÑ ÿ≠ÿ¨ŸÖ ÿßŸÑÿØŸèŸÅÿπÿ©                            |\n",
    "| **No GPU detected**        | Install `tensorflow[and-cuda]` or check CUDA/cuDNN / ÿ´ÿ®Ÿëÿ™ tensorflow ŸÖÿπ CUDA |\n",
    "| **Slow training**          | Ensure GPU is being used (check Cell 2 output) / ÿ™ÿ£ŸÉÿØ ÿ•ŸÜ ÿßŸÑŸÄ GPU ÿ¥ÿ∫ŸëÿßŸÑ       |\n",
    "| **Low accuracy**           | Increase epochs, add more data, or tune LSTM units / ÿ≤ŸàŸëÿØ ÿßŸÑÿ≠ŸÇÿ® ÿ£Ÿà ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™  |\n",
    "| **Mixed precision errors** | Remove the mixed precision block in Cell 2 / ÿßÿ≠ÿ∞ŸÅ ŸÉŸàÿØ ÿßŸÑÿØŸÇÿ© ÿßŸÑŸÖÿÆÿ™ŸÑÿ∑ÿ©         |\n",
    "\n",
    "### Monitor GPU / ŸÖÿ±ÿßŸÇÿ®ÿ© ÿßŸÑŸÄ GPU:\n",
    "\n",
    "```powershell\n",
    "nvidia-smi -l 1\n",
    "```\n",
    "\n",
    "### Key Differences from Letter Training / ÿßŸÑŸÅÿ±ŸàŸÇÿßÿ™ ÿπŸÜ ÿ™ÿØÿ±Ÿäÿ® ÿßŸÑÿ≠ÿ±ŸàŸÅ:\n",
    "\n",
    "- Letters use **MLP** (flat keypoints per image) / ÿßŸÑÿ≠ÿ±ŸàŸÅ ÿ™ÿ≥ÿ™ÿÆÿØŸÖ MLP (ŸÜŸÇÿßÿ∑ ŸÖÿ≥ÿ∑ÿ≠ÿ© ŸÑŸÉŸÑ ÿµŸàÿ±ÿ©)\n",
    "- Words use **BiLSTM** (sequences of keypoints over time) / ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ™ÿ≥ÿ™ÿÆÿØŸÖ BiLSTM (ÿ™ÿ≥ŸÑÿ≥ŸÑÿßÿ™ ŸÜŸÇÿßÿ∑ ÿπÿ®ÿ± ÿßŸÑÿ≤ŸÖŸÜ)\n",
    "- Words need `SEQUENCE_LENGTH` frames per sample / ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ™ÿ≠ÿ™ÿßÿ¨ ÿπÿØÿØ ÿ•ÿ∑ÿßÿ±ÿßÿ™ ÿ´ÿßÿ®ÿ™ ŸÑŸÉŸÑ ÿπŸäŸÜÿ©\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
